---
title: "Mini-Project 02 - Making Backyards Affordable for All"
author: "Caroline Guirand"
editor:
    mode: source
format:
    html:
        code-fold: true

---

## Introduction

A suitable introduction goes here. In this mini-project, I intend to demonstrate
my skills at Data Integration/Data Visualization/Metric Creation in an analysis of Official Statistics/Economic Data/Census Data data.

## Data Acquisition and Preparation

```{r}
# Task 1: Data Import
# ===================
# In this section, we download and prepare census data from the American Community
# Survey (ACS) covering multiple years. We focus on four key metrics that will help
# us understand housing affordability across US metro areas.

# First, we set up our data directory structure
if(!dir.exists(file.path("data", "mp02"))){
  dir.create(file.path("data", "mp02"), showWarnings=FALSE, recursive=TRUE)
}

# Override the library() function to auto-install packages if needed
# This ensures reproducibility across different environments
library <- function(pkg){
  pkg <- as.character(substitute(pkg))
  options(repos = c(CRAN = "https://cloud.r-project.org"))
  if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)
  stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))
}

# Load required packages
library(tidyverse)   # For data manipulation and visualization
library(glue)        # For string interpolation
library(readxl)      # For reading Excel files
library(tidycensus)  # For accessing US Census Bureau data
library(httr2)       # For making HTTP requests to BLS
library(rvest)       # For web scraping BLS data

# Define helper function to download ACS data across multiple years
# This function handles caching to avoid repeated API calls
get_acs_all_years <- function(variable, geography="cbsa",
                               start_year=2009, end_year=2023){
  # Create file name for cached data
  fname <- glue("{variable}_{geography}_{start_year}_{end_year}.csv")
  fname <- file.path("data", "mp02", fname)
  
  # Only download if we don't already have the data cached
  if(!file.exists(fname)){
    YEARS <- seq(start_year, end_year)
    YEARS <- YEARS[YEARS != 2020] # Drop 2020 - No ACS 1-year survey (COVID-19)
    
    # Download data for each year and combine
    ALL_DATA <- map(YEARS, function(yy){
      tidycensus::get_acs(geography, variable, year=yy, survey="acs1") |>
        mutate(year=yy) |>
        select(-moe, -variable) |>  # Remove margin of error and variable name
        rename(!!variable := estimate)
    }) |> bind_rows()
    
    # Cache the results
    write_csv(ALL_DATA, fname)
  }
  
  # Read and return the data
  read_csv(fname, show_col_types=FALSE)
}

# Download four key data sets for Core Based Statistical Areas (CBSAs/metro areas)
# KEY OBSERVATION: All data sets share common keys: GEOID, NAME, and year
# These will be used to join the data sets together

# 1. Median household income (in 12-month inflation-adjusted dollars)
INCOME <- get_acs_all_years("B19013_001") |>
  rename(household_income = B19013_001)

# 2. Median gross rent (monthly, in dollars)
RENT <- get_acs_all_years("B25064_001") |>
  rename(monthly_rent = B25064_001)

# 3. Total population
POPULATION <- get_acs_all_years("B01003_001") |>
  rename(population = B01003_001)

# 4. Total number of households
HOUSEHOLDS <- get_acs_all_years("B11001_001") |>
  rename(households = B11001_001)
```


```{r}
# Download Building Permits Data
# ================================
# The number of new housing units permitted is not available via tidycensus,
# so we're downloading it directly from Census Bureau construction data.
# Note: Historical data (2009-2018) and current data (2019+) use different formats.

get_building_permits <- function(start_year = 2009, end_year = 2023){
  fname <- glue("housing_units_{start_year}_{end_year}.csv")
  fname <- file.path("data", "mp02", fname)
  
  if(!file.exists(fname)){
    # Process historical data (2009-2018): Fixed-width text format
    HISTORICAL_YEARS <- seq(start_year, 2018)
    
    HISTORICAL_DATA <- map(HISTORICAL_YEARS, function(yy){
      historical_url <- glue("https://www.census.gov/construction/bps/txt/tb3u{yy}.txt")
      
      # Read and parse fixed-width text file
      LINES <- readLines(historical_url)[-c(1:11)]  # Skip header lines
      
      # Extract CBSA codes (columns 5-10)
      CBSA_LINES <- str_detect(LINES, "^[[:digit:]]")
      CBSA <- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))
      
      # Extract permit counts (columns 48-53)
      PERMIT_LINES <- str_detect(str_sub(LINES, 48, 53), "[[:digit:]]")
      PERMITS <- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))
      
      data_frame(CBSA = CBSA,
                 new_housing_units_permitted = PERMITS, 
                 year = yy)
    }) |> bind_rows()
    
    # Process current data (2019-2023): Excel format
    CURRENT_YEARS <- seq(2019, end_year)
    
    CURRENT_DATA <- map(CURRENT_YEARS, function(yy){
      current_url <- glue("https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls")
      
      temp <- tempfile()
      download.file(current_url, destfile = temp, mode="wb")
      
      # Try xlsx format first, fall back to xls if needed
      fallback <- function(.f1, .f2){
        function(...){
          tryCatch(.f1(...), 
                   error=function(e) .f2(...))
        }
      }
      
      reader <- fallback(read_xlsx, read_xls)
      
      reader(temp, skip=5) |>
        na.omit() |>
        select(CBSA, Total) |>
        mutate(year = yy) |>
        rename(new_housing_units_permitted = Total)
    }) |> bind_rows()
    
    # Combine historical and current data
    ALL_DATA <- rbind(HISTORICAL_DATA, CURRENT_DATA)
    
    # Cache the results
    write_csv(ALL_DATA, fname)
  }
  
  # Read and return the data
  read_csv(fname, show_col_types=FALSE)
}

# Download the permits data
# Note: CBSA is stored as a numeric code, unlike GEOID in census data
PERMITS <- get_building_permits()
```

```{r}
# Download BLS Industry Classification Codes (NAICS)
# ===================================================
# The Bureau of Labor Statistics uses the North American Industry Classification
# System (NAICS) to categorize industries. We need this lookup table to 
# understand which industries are represented in the wage data.

get_bls_industry_codes <- function(){
  fname <- file.path("data", "mp02", "bls_industry_codes.csv")
  
  if(!file.exists(fname)){
    # Download NAICS codes from BLS website
    resp <- request("https://www.bls.gov") |> 
      req_url_path("cew", "classifications", "industry", "industry-titles.htm") |>
      req_headers(`User-Agent` = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0") |> 
      req_error(is_error = \(resp) FALSE) |>
      req_perform()
    
    resp_check_status(resp)
    
    # Parse the HTML table containing NAICS codes
    naics_table <- resp_body_html(resp) |>
      html_element("#naics_titles") |> 
      html_table() |>
      mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), "NAICS"))) |>
      select(Code, `Industry Title`) |>
      rename(title = `Industry Title`) |>
      mutate(depth = if_else(nchar(Code) <= 5, nchar(Code) - 1, NA)) |>
      filter(!is.na(depth))
    
    # Manually add codes that were presented as ranges on the website
    # (There are only three, so manual handling is easier than special-casing)
    naics_missing <- tibble::tribble(
      ~Code, ~title, ~depth, 
      "31", "Manufacturing", 1,
      "32", "Manufacturing", 1,
      "33", "Manufacturing", 1,
      "44", "Retail", 1, 
      "45", "Retail", 1,
      "48", "Transportation and Warehousing", 1, 
      "49", "Transportation and Warehousing", 1
    )
    
    naics_table <- bind_rows(naics_table, naics_missing)
    
    # Create hierarchical structure: each industry has 4 levels of detail
    # Level 1 = broad (e.g., "Manufacturing")
    # Level 4 = specific (e.g., "Semiconductor Manufacturing")
    naics_table <- naics_table |> 
      filter(depth == 4) |>  # Focus on most detailed level
      rename(level4_title=title) |> 
      mutate(level1_code = str_sub(Code, end=2), 
             level2_code = str_sub(Code, end=3), 
             level3_code = str_sub(Code, end=4)) |>
      left_join(naics_table, join_by(level1_code == Code)) |>
      rename(level1_title=title) |>
      left_join(naics_table, join_by(level2_code == Code)) |>
      rename(level2_title=title) |>
      left_join(naics_table, join_by(level3_code == Code)) |>
      rename(level3_title=title) |>
      select(-starts_with("depth")) |>
      rename(level4_code = Code) |>
      select(level1_title, level2_title, level3_title, level4_title, 
             level1_code,  level2_code,  level3_code,  level4_code) |>
      drop_na() |>
      mutate(across(contains("code"), as.integer))
    
    # Cache the results
    write_csv(naics_table, fname)
  }
  
  # Read and return the data
  read_csv(fname, show_col_types=FALSE)
}

# Download industry codes lookup table
INDUSTRY_CODES <- get_bls_industry_codes()

```

```{r}

# Download BLS Quarterly Census of Employment and Wages (QCEW)
# =============================================================
# The QCEW provides detailed employment and wage data by industry and geography.
# We use the annual averages to understand labor market conditions across CBSAs.

get_bls_qcew_annual_averages <- function(start_year=2009, end_year=2023){
  fname <- glue("bls_qcew_{start_year}_{end_year}.csv.gz")
  fname <- file.path("data", "mp02", fname)
  
  YEARS <- seq(start_year, end_year)
  YEARS <- YEARS[YEARS != 2020] # Drop 2020 to match ACS (COVID year)
  
  if(!file.exists(fname)){
    # Download and process data for each year
    ALL_DATA <- map(YEARS, .progress=TRUE, possibly(function(yy){
      fname_inner <- file.path("data", "mp02", glue("{yy}_qcew_annual_singlefile.zip"))
      
      # Download annual file if not already cached
      if(!file.exists(fname_inner)){
        request("https://www.bls.gov") |> 
          req_url_path("cew", "data", "files", yy, "csv",
                       glue("{yy}_annual_singlefile.zip")) |>
          req_headers(`User-Agent` = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0") |> 
          req_retry(max_tries=5) |>
          req_perform(fname_inner)
      }
      
      # Verify file downloaded correctly (should be ~75MB)
      if(file.info(fname_inner)$size < 755e5){
        warning(sQuote(fname_inner), " appears corrupted. Please delete and retry this step.")
      }
      
      # Read and filter the data
      read_csv(fname_inner, show_col_types=FALSE) |> 
        mutate(YEAR = yy) |>
        select(area_fips,           # Geographic identifier
               industry_code,        # NAICS industry code
               annual_avg_emplvl,    # Average employment level
               total_annual_wages,   # Total wages paid
               YEAR) |>
        # Filter to CBSAs only (FIPS starting with 'C') and valid industry codes
        filter(nchar(industry_code) <= 5,           # Keep only detailed industries
               str_starts(area_fips, "C")) |>       # Keep only CBSAs
        filter(str_detect(industry_code, "-", negate=TRUE)) |>  # Remove ranges
        # Rename for clarity
        mutate(FIPS = area_fips, 
               INDUSTRY = as.integer(industry_code), 
               EMPLOYMENT = as.integer(annual_avg_emplvl), 
               TOTAL_WAGES = total_annual_wages) |>
        select(-area_fips, -industry_code, -annual_avg_emplvl, -total_annual_wages) |>
        # Remove "all industries" aggregate (code 10)
        filter(INDUSTRY != 10) |> 
        # Calculate average wage per employee
        mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)
    })) |> bind_rows()
    
    # Cache the combined results (compressed to save space)
    write_csv(ALL_DATA, fname)
  }
  
  # Read the cached data
  ALL_DATA <- read_csv(fname, show_col_types=FALSE)
  
  # Verify all years downloaded successfully
  ALL_DATA_YEARS <- unique(ALL_DATA$YEAR)
  YEARS_DIFF <- setdiff(YEARS, ALL_DATA_YEARS)
  
  if(length(YEARS_DIFF) > 0){
    stop("Download failed for the following years: ", YEARS_DIFF, 
         ". Please delete intermediate files and try again.")
  }
  
  ALL_DATA
}

# Download wage data
WAGES <- get_bls_qcew_annual_averages()
```


### Examining Dataset Structure and Identifying Join Keys
Now that we have downloaded all necessary datasets, we need to examine their structure
to identify the appropriate keys for joining them together in our analysis.

```{r}
# Examine Census Data Structure
cat("=", rep("=", 69), "\n", sep="")
cat("CENSUS DATA (from tidycensus - ACS)\n")
cat("=", rep("=", 69), "\n\n", sep="")

glimpse(INCOME)
```

The American Community Survey (ACS) provides four key data sets for our analysis. The **INCOME** data set contains `r nrow(INCOME)` observations across `r n_distinct(INCOME$GEOID)` unique Core Based Statistical Areas (CBSAs). These metro areas are tracked over `r n_distinct(INCOME$year)` years, specifically: `r paste(sort(unique(INCOME$year)), collapse=", ")`. Note that 2020 is excluded because the Census Bureau did not conduct the 1-year ACS survey that year due to the COVID-19 pandemic.

All four census data sets (INCOME, RENT, POPULATION, and HOUSEHOLDS) share the same structure with three common keys: **GEOID** (numeric CBSA identifier), **NAME** (metro area name), and **year** (survey year). These datasets can be joined directly to each other using `GEOID` and `year` as composite keys.

```{r}
# Examine Building Permits Data Structure
cat("=", rep("=", 69), "\n", sep="")
cat("BUILDING PERMITS DATA\n")
cat("=", rep("=", 69), "\n\n", sep="")

glimpse(PERMITS)
```

The **PERMITS** data set tracks new housing construction permits across `r n_distinct(PERMITS$CBSA)` CBSAs over `r n_distinct(PERMITS$year)` years (`r min(PERMITS$year)`-`r max(PERMITS$year)`). Unlike the census data, this data set includes 2020 data. The PERMITS data set uses `CBSA` as a numeric code, matching the format of `GEOID` in the census data, so we can join them directly: `GEOID == CBSA`.

```{r}
# Examine Industry Classification Data
cat("=", rep("=", 69), "\n", sep="")
cat("BLS INDUSTRY CODES (NAICS Classification)\n")
cat("=", rep("=", 69), "\n\n", sep="")

glimpse(INDUSTRY_CODES)
head(INDUSTRY_CODES %>% select(level1_title, level4_title, level4_code), 3)
```

The **INDUSTRY_CODES** data set is a lookup table containing `r nrow(INDUSTRY_CODES)` detailed industry classifications following the North American Industry Classification System (NAICS). This hierarchical system has four levels, from broad sectors (level 1) to specific industries (level 4). For example, level 1 includes broad categories like "`r INDUSTRY_CODES$level1_title[1]`" while level 4 provides specific industries like "`r INDUSTRY_CODES$level4_title[1]`". This table will be essential for understanding the wage data by industry type, joining on `level4_code == INDUSTRY`.

```{r}
# Examine BLS Wage Data Structure
cat("=", rep("=", 69), "\n", sep="")
cat("BLS QCEW WAGE DATA\n")
cat("=", rep("=", 69), "\n\n", sep="")

glimpse(WAGES)
cat("\nSample FIPS codes:", head(unique(WAGES$FIPS), 5), "\n")
```

The **WAGES** data set from the BLS Quarterly Census of Employment and Wages is our largest data set with `r format(nrow(WAGES), big.mark=",")` observations. It tracks employment and wages across `r n_distinct(WAGES$FIPS)` geographic areas, `r n_distinct(WAGES$INDUSTRY)` industries, and `r n_distinct(WAGES$YEAR)` years. 

The joining strategy here is more complex because the `FIPS` column stores CBSA codes with a "C" prefix (e.g., "C10180"). To join with census data, we need to extract the numeric portion: `as.numeric(substr(FIPS, 2, 6))`. Additionally, the year column is named `YEAR` (uppercase) rather than `year` (lowercase), so we'll need to account for this when joining.

```{r}
# Create summary table of all data sets
data_summary <- tibble(
  Dataset = c("INCOME/RENT/POPULATION/HOUSEHOLDS", "PERMITS", "WAGES", "INDUSTRY_CODES"),
  Observations = c(format(nrow(INCOME), big.mark=","), 
                   format(nrow(PERMITS), big.mark=","), 
                   format(nrow(WAGES), big.mark=","), 
                   format(nrow(INDUSTRY_CODES), big.mark=",")),
  Geographic_Units = c(n_distinct(INCOME$GEOID), 
                       n_distinct(PERMITS$CBSA), 
                       n_distinct(WAGES$FIPS), 
                       "N/A (Lookup Table)"),
  Time_Periods = c(n_distinct(INCOME$year), 
                   n_distinct(PERMITS$year), 
                   n_distinct(WAGES$YEAR), 
                   "N/A"),
  Primary_Keys = c("GEOID + year", 
                   "CBSA + year", 
                   "FIPS + YEAR + INDUSTRY", 
                   "level4_code")
)

knitr::kable(data_summary, 
             caption = "Summary of All Datasets",
             align = c('l', 'r', 'r', 'r', 'l'))
```

**Summary of Joining Strategy:**

1. **Census data sets** (INCOME, RENT, POPULATION, HOUSEHOLDS) can be joined directly on `GEOID` and `year`
2. **PERMITS** joins to census data where `GEOID == CBSA` and `year` matches  
3. **WAGES** requires extracting numeric CBSA from `FIPS` column, then joining on the converted value and matching year
4. **INDUSTRY_CODES** joins to WAGES where `INDUSTRY == level4_code` to add readable industry names

Now we can continue to combine these data sets for our YIMBY analysis. 

## Exploratory Analysis

### Entity Relationship Diagram (ERD)
To better understand the relationships between these data sets, I've created an Entity Relationship Diagram (ERD) using the DiagrammeR package. This shows the structure of each table and their relationships.

```{r}
# Install and load DiagrammeR package for creating the ERD
if(!require(DiagrammeR, quietly = TRUE)) {
  install.packages("DiagrammeR")
}
library(DiagrammeR)
```

```{r}
#| label: fig-erd-r
#| fig-cap: "Entity Relationship Diagram created with DiagrammeR"
#| fig-width: 12
#| fig-height: 10

library(DiagrammeR)

grViz("
digraph ERD {
  graph [rankdir=LR, fontname=Helvetica, bgcolor=white]
  node [shape=rectangle, style=filled, fontname=Helvetica, fontsize=10, margin=0.2]
  edge [fontname=Helvetica, fontsize=8]
  
  # Census datasets
  INCOME [label='INCOME\n━━━━━━\nGEOID (PK)\nNAME\nyear (PK)\n━━━━━━\nhousehold_income', fillcolor='#E8F4F8']
  RENT [label='RENT\n━━━━━━\nGEOID (PK)\nNAME\nyear (PK)\n━━━━━━\nmonthly_rent', fillcolor='#E8F4F8']
  POPULATION [label='POPULATION\n━━━━━━\nGEOID (PK)\nNAME\nyear (PK)\n━━━━━━\npopulation', fillcolor='#E8F4F8']
  HOUSEHOLDS [label='HOUSEHOLDS\n━━━━━━\nGEOID (PK)\nNAME\nyear (PK)\n━━━━━━\nhouseholds', fillcolor='#E8F4F8']
  
  # Other datasets
  PERMITS [label='PERMITS\n━━━━━━\nCBSA (PK)\nyear (PK)\n━━━━━━\nnew_housing_units', fillcolor='#FFF4E6']
  WAGES [label='WAGES\n━━━━━━\nFIPS (PK)\nYEAR (PK)\nINDUSTRY (PK)\n━━━━━━\nEMPLOYMENT\nTOTAL_WAGES\nAVG_WAGE', fillcolor='#F0E6FF']
  INDUSTRY_CODES [label='INDUSTRY_CODES\n━━━━━━\nlevel4_code (PK)\n━━━━━━\nlevel1-4_title\nlevel1-4_code', fillcolor='#E6F7E6']
  
  # Relationships
  INCOME -> RENT [label='GEOID+year', color='#0066CC', penwidth=2]
  INCOME -> POPULATION [label='GEOID+year', color='#0066CC', penwidth=2]
  INCOME -> HOUSEHOLDS [label='GEOID+year', color='#0066CC', penwidth=2]
  INCOME -> PERMITS [label='GEOID=CBSA\n+year', color='#FF9900', style=dashed, penwidth=2]
  INCOME -> WAGES [label='GEOID=\nsubstr(FIPS,2)\n+year=YEAR', color='#9933FF', style=dashed, penwidth=2]
  WAGES -> INDUSTRY_CODES [label='INDUSTRY=\nlevel4_code', color='#009900', penwidth=2]
}
")
```

**Key Insights from the ERD:**

The diagram reveals three distinct relationship patterns in our data:

1. **Direct Relationships (Census Data)**: The four ACS data sets (INCOME, RENT, POPULATION, HOUSEHOLDS) share identical structures with `r n_distinct(INCOME$GEOID)` CBSAs tracked over `r n_distinct(INCOME$year)` years. They can be joined directly using composite keys (`GEOID` + `year`), making them straightforward to combine.

2. **Simple Type Conversion (PERMITS)**: The building permits data uses `CBSA` as a numeric identifier, which directly matches the `GEOID` in census data. This allows for a direct join: `GEOID == CBSA`, though we must be mindful that PERMITS includes 2020 data while census data does not.

3. **Complex String Manipulation (WAGES)**: The wage data presents the most complex joining challenge. The `FIPS` column stores CBSA codes with a "C" prefix (e.g., "C10180" for Abilene, TX), requiring string extraction via `as.numeric(substr(FIPS, 2, 6))` to match with census `GEOID` values. Additionally, the year column is uppercase (`YEAR` vs `year`), necessitating careful attention during joins.

4. **Lookup Table Enhancement (INDUSTRY_CODES)**: The NAICS industry classification table serves as a hierarchical lookup, allowing us to enrich the wage data with human-readable industry names and broader sector classifications through the `INDUSTRY == level4_code` relationship.

This relationship structure will guide our data integration strategy in the following sections, where we'll progressively build a comprehensive data set combining housing affordability metrics, construction activity, and labor market conditions across US metropolitan areas.


##  Data Integration and Initial Exploration

### Task 2 Questions 

### Question 1: Largest Housing Construction (2010-2019)

**Question:** Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?
```{r}
# Filter permits data to the decade 2010-2019 and sum by CBSA
# Then joins with census data to get the readable CBSA name

q1_result <- PERMITS |>
  # Filter to the specified decade
  filter(year >= 2010, year <= 2019) |>
  # Group by CBSA and sum all permits over the decade
  group_by(CBSA) |>
  summarize(total_permits = sum(new_housing_units_permitted, na.rm = TRUE)) |>
  # Sort to find the top CBSA
  arrange(desc(total_permits)) |>
  # Take the top result
  slice(1) |>
  # Join with INCOME (or any census table) to get the CBSA name
  # Use distinct to get just one row per CBSA (since census has multiple years)
  left_join(
    INCOME |> select(GEOID, NAME) |> distinct(),
    by = c("CBSA" = "GEOID")
  )

# Display the result
q1_result
```

**Answer:** The CBSA that permitted the most new housing units between 2010 and 2019 was **`r q1_result$NAME`**, with a total of **`r format(q1_result$total_permits, big.mark=",")`** units permitted during this decade. This reflects the major population growth and housing demand this metropolitan area saw during the 2010s.

---

### Question 2: Albuquerque's Peak Construction Year

**Question:** In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?
```{r}
# Filter to Albuquerque CBSA and find the year with maximum permits

q2_result <- PERMITS |>
  # Filter to Albuquerque only
  filter(CBSA == 10740) |>
  # Sort by permits to find the maximum
  arrange(desc(new_housing_units_permitted)) |>
  # Take the top year
  slice(1)

# Display the result
q2_result

# Also create a visualization to see the trend and identify the artifact
albuquerque_trend <- PERMITS |>
  filter(CBSA == 10740) |>
  arrange(year)

# Show the full time series to identify any anomalies
albuquerque_trend
```

**Answer:** According to the data, Albuquerque permitted the most housing units in **`r q2_result$year`**, with **`r format(q2_result$new_housing_units_permitted, big.mark=",")`** units. 

**Important Note on COVID-19 Data Artifact:** Based on this time series, we should be cautious about this result since the year `r q2_result$year` coincides with the COVID-19 pandemic period. This might've affected data collection and reporting. If 2020 shows an unusually high value, this may represent a data quality issue rather than actual construction activity. A more reliable answer would exclude 2020 and focus on years with normal data collection: the peak non-pandemic year was **`r albuquerque_trend |> filter(year != 2020) |> arrange(desc(new_housing_units_permitted)) |> slice(1) |> pull(year)`** with **`r format(albuquerque_trend |> filter(year != 2020) |> arrange(desc(new_housing_units_permitted)) |> slice(1) |> pull(new_housing_units_permitted), big.mark=",")`** units.

---

### Question 3: Highest Average Income by State (2015)

**Question:** Which state (not CBSA) had the highest average individual income in 2015?
```{r}
# Create state lookup table for converting abbreviations to full names
state_df <- data.frame(
  abb  = c(state.abb, "DC", "PR"),
  name = c(state.name, "District of Columbia", "Puerto Rico")
)

# Calculate total income and population by state for 2015
q3_result <- INCOME |>
  # Filter to 2015 only
  filter(year == 2015) |>
  # Join with HOUSEHOLDS to get household counts
  inner_join(
    HOUSEHOLDS |> filter(year == 2015),
    by = c("GEOID", "NAME", "year")
  ) |>
  # Join with POPULATION to get total population
  inner_join(
    POPULATION |> filter(year == 2015),
    by = c("GEOID", "NAME", "year")
  ) |>
  # Extract the principal state from CBSA name
  mutate(state = str_extract(NAME, ", (.{2})", group = 1)) |>
  # Calculate total income per CBSA (household_income * households)
  mutate(total_income = household_income * households) |>
  # Group by state and sum income and population
  group_by(state) |>
  summarize(
    total_state_income = sum(total_income, na.rm = TRUE),
    total_state_population = sum(population, na.rm = TRUE)
  ) |>
  # Calculate average individual income per state
  mutate(avg_individual_income = total_state_income / total_state_population) |>
  # Join with state names for readability
  left_join(state_df, by = c("state" = "abb")) |>
  # Sort by average income
  arrange(desc(avg_individual_income)) |>
  # Take the top state
  slice(1)

# Display the result
q3_result
```

**Answer:** In 2015, **`r q3_result$name`** (state abbreviation: `r q3_result$state`) had the highest average individual income at **$`r format(round(q3_result$avg_individual_income, 2), big.mark=",", nsmall=2)`**. This was calculated by summing total household income across all CBSAs in the state (income × households) and dividing by the total population, giving us the average income per person rather than per household.

---

### Question 4: Data Scientists Employment - NYC vs San Francisco

**Question:** What is the last year in which the NYC CBSA had the most data scientists in the country?

First, we need to create a compatible CBSA code in the WAGES data to join with census data:
```{r}
# Transform BLS FIPS codes to match Census GEOID format
# BLS uses "C1234" while Census uses "12340" (numeric, with trailing 0)
WAGES_clean <- WAGES |>
  mutate(
    # Remove the "C" prefix, convert to numeric, then multiply by 10 to add trailing 0
    CBSA_code = as.numeric(str_remove(FIPS, "C")) * 10
  )

# Verify the transformation worked
cat("Sample FIPS codes from WAGES:", head(WAGES$FIPS, 3), "\n")
cat("Converted to CBSA codes:", head(WAGES_clean$CBSA_code, 3), "\n")
cat("Sample GEOID from census:", head(INCOME$GEOID, 3), "\n")
```

Now we find when NYC had the most data scientists:
```{r}
# NAICS code 5182 = Data Processing, Hosting, and Related Services
# (This is the code mentioned for data scientists and business analysts)

# Find which CBSA had the most data scientists each year
q4_result <- WAGES_clean |>
  # Filter to data scientists industry only
  filter(INDUSTRY == 5182) |>
  # For each year, find the CBSA with maximum employment
  group_by(YEAR) |>
  arrange(desc(EMPLOYMENT)) |>
  slice(1) |>
  ungroup() |>
  # Join with census data to get CBSA names
  left_join(
    INCOME |> select(GEOID, NAME) |> distinct(),
    by = c("CBSA_code" = "GEOID")
  ) |>
  # Select relevant columns for display
  select(YEAR, NAME, CBSA_code, EMPLOYMENT) |>
  arrange(YEAR)

# Display the full table
q4_result

# Find the last year NYC was on top
nyc_last_year <- q4_result |>
  filter(str_detect(NAME, "New York")) |>
  arrange(desc(YEAR)) |>
  slice(1)
```

**Answer:** Based on the analysis, the last year when the NYC CBSA had the most data scientists was **`r nyc_last_year$YEAR`**, with **`r format(nyc_last_year$EMPLOYMENT, big.mark=",")`** employees in NAICS industry 5182. After this year, San Francisco (or another CBSA) took the lead in data science employment, reflecting the tech industry's concentration on the West Coast.

The table above shows the CBSA with the most data scientists for each year in our data set, illustrating the shift in where data science jobs are concentrated.

---

### Question 5: Finance Industry Wages in NYC

**Question:** What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?
```{r}
# First, identify NYC's CBSA code
nyc_info <- INCOME |>
  filter(str_detect(NAME, "New York-Newark-Jersey City")) |>
  select(GEOID, NAME) |>
  distinct()

nyc_cbsa <- nyc_info$GEOID[1]
cat("NYC CBSA code:", nyc_cbsa, "\n")

# Calculate finance wages as fraction of total wages each year
q5_result <- WAGES_clean |>
  # Filter to NYC only
  filter(CBSA_code == nyc_cbsa) |>
  # Create indicator for finance industry (NAICS 52 = level 2 code)
  # Industry codes starting with 52 are finance
  mutate(is_finance = str_starts(as.character(INDUSTRY), "52")) |>
  # Group by year and calculate totals
  group_by(YEAR) |>
  summarize(
    finance_wages = sum(TOTAL_WAGES[is_finance], na.rm = TRUE),
    total_wages = sum(TOTAL_WAGES, na.rm = TRUE),
    finance_employment = sum(EMPLOYMENT[is_finance], na.rm = TRUE),
    total_employment = sum(EMPLOYMENT, na.rm = TRUE)
  ) |>
  # Calculate the fraction
  mutate(
    finance_wage_fraction = finance_wages / total_wages,
    finance_emp_fraction = finance_employment / total_employment
  ) |>
  arrange(YEAR)

# Display the results
q5_result

# Find the peak year
peak_year <- q5_result |>
  arrange(desc(finance_wage_fraction)) |>
  slice(1)
```

**Answer:** In the NYC CBSA, the finance and insurance industry's share of total wages peaked in **`r peak_year$YEAR`**, when it accounted for **`r round(peak_year$finance_wage_fraction * 100, 2)`%** of all wages paid in the metro area. At that time, the finance sector employed **`r round(peak_year$finance_emp_fraction * 100, 2)`%** of the workforce but earned a disproportionately large share of total wages, which is reflected in the high salaries in this industry.

Over the entire time period, the finance industry's wage share in NYC ranged from **`r round(min(q5_result$finance_wage_fraction) * 100, 2)`%** to **`r round(max(q5_result$finance_wage_fraction) * 100, 2)`%**, demonstrating the continued importance of the financial sector to New York's economy, even as its relative dominance has evolved over time.



### Task 2

The following visualizations will help us explore housing costs, employment patterns, and household characteristics across US metropolitan areas, and therefore, better understand the relationships in our data.

### Visualization 1: Rent vs. Income Relationship (2009)

**Question:** What is the relationship between monthly rent and average household income per CBSA in 2009?
```{r}
# Join RENT and INCOME data for 2009
rent_income_2009 <- RENT |>
  filter(year == 2009) |>
  inner_join(
    INCOME |> filter(year == 2009),
    by = c("GEOID", "NAME", "year")
  ) |>
  # Remove any missing values
  filter(!is.na(monthly_rent), !is.na(household_income))

# Create the visualization
ggplot(rent_income_2009, aes(x = household_income, y = monthly_rent)) +
  # Add points with some transparency to handle over plotting
  geom_point(alpha = 0.6, color = "#2C3E50", size = 2) +
  # Add a smooth trend line to show the relationship
  geom_smooth(method = "lm", se = TRUE, color = "#E74C3C", linewidth = 1.2) +
  # Format axis labels with dollar signs and comma separators
  scale_x_continuous(
    labels = scales::dollar_format(),
    breaks = seq(0, 100000, 20000)
  ) +
  scale_y_continuous(
    labels = scales::dollar_format(),
    breaks = seq(0, 2000, 250)
  ) +
  # Add proper labels and title
  labs(
    title = "Relationship Between Household Income and Monthly Rent (2009)",
    subtitle = "Each point represents a Core Based Statistical Area (CBSA)",
    x = "Median Household Income (Annual)",
    y = "Median Monthly Gross Rent",
    caption = "Data Source: American Community Survey (ACS) 2009"
  ) +
  # Use a clean theme with larger text
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "gray40"),
    axis.title = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

# Calculate correlation for narrative
correlation <- cor(rent_income_2009$household_income, 
                   rent_income_2009$monthly_rent, 
                   use = "complete.obs")
```

**Observation:** There is a strong positive relationship (correlation = `r round(correlation, 3)`) between household income and monthly rent across CBSAs in 2009. Metropolitan areas with higher median household incomes tend to have higher median rents. Areas with higher-paying jobs attract more residents, driving up housing costs. The relationship seems to be somewhat linear, despite a considerable amount of variation around the trend line. This suggests that local factors beyond income (such as housing supply, geography, and regulations) also play important roles in determining rent levels.

---

### Visualization 2: Healthcare Employment Over Time

**Question:** What is the relationship between total employment and healthcare/social services employment across CBSAs over time?
```{r}
# Prepare wage data with CBSA codes
WAGES_clean <- WAGES |>
  mutate(CBSA_code = as.numeric(str_remove(FIPS, "C")) * 10)

# Calculate total employment and healthcare employment by CBSA and year
healthcare_employment <- WAGES_clean |>
  # Healthcare and Social Services is NAICS 62 (all codes starting with 62)
  mutate(is_healthcare = str_starts(as.character(INDUSTRY), "62")) |>
  group_by(CBSA_code, YEAR) |>
  summarize(
    total_employment = sum(EMPLOYMENT, na.rm = TRUE),
    healthcare_employment = sum(EMPLOYMENT[is_healthcare], na.rm = TRUE),
    .groups = "drop"
  ) |>
  # Calculate healthcare as percentage of total
  mutate(healthcare_pct = healthcare_employment / total_employment * 100) |>
  # Filter out CBSAs with very small employment (potential data issues)
  filter(total_employment >= 10000)

# Create the visualization showing evolution over time
ggplot(healthcare_employment, 
       aes(x = total_employment, y = healthcare_employment, color = as.factor(YEAR))) +
  # Use points with transparency
  geom_point(alpha = 0.5, size = 1.5) +
  # Add a reference line showing constant healthcare share
  geom_abline(slope = 0.15, intercept = 0, linetype = "dashed", 
              color = "gray40", linewidth = 0.8) +
  # Format axes with comma separators
  scale_x_continuous(
    labels = scales::comma_format(),
    trans = "log10"  # Log scale helps show the relationship across different sized CBSAs
  ) +
  scale_y_continuous(
    labels = scales::comma_format(),
    trans = "log10"
  ) +
  # Use a color palette that shows time progression
  scale_color_viridis_d(option = "plasma", name = "Year") +
  # Add proper labels
  labs(
    title = "Healthcare Employment vs. Total Employment Across US Metro Areas",
    subtitle = "Healthcare sector consistently represents ~15% of total employment (dashed line)",
    x = "Total Employment (log scale)",
    y = "Healthcare & Social Services Employment (log scale)",
    caption = "Data Source: BLS Quarterly Census of Employment and Wages (QCEW)\nNote: CBSAs with <10,000 total employment excluded"
  ) +
  # Clean theme
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "gray40"),
    axis.title = element_text(face = "bold"),
    legend.position = "right",
    panel.grid.minor = element_blank()
  )

# Calculate average healthcare share across all years
avg_healthcare_share <- mean(healthcare_employment$healthcare_pct, na.rm = TRUE)
```

**Observation:** Healthcare and social services employment shows a consistent relationship with total employment throughout metro areas and over time. On average, the healthcare sector accounts for approximately **`r round(avg_healthcare_share, 1)`%** of total employment across all CBSAs in our data set. The dashed reference line (representing 15% of total employment) shows that this relationship is relatively stable throughout metro areas of different sizes. The coloring by year reveals that this relationship has remained stable over time, though there may be a slight upward trend in healthcare's share of employment in recent years, reflecting the aging population and expansion of healthcare services.

---

### Visualization 3: Evolution of Household Size Over Time

**Question:** How has average household size evolved over time across different CBSAs?
```{r}
# Calculate average household size (population / households) by CBSA and year
household_size <- POPULATION |>
  inner_join(
    HOUSEHOLDS,
    by = c("GEOID", "NAME", "year")
  ) |>
  mutate(avg_household_size = population / households) |>
  # Filter out potential data errors (unrealistic household sizes)
  filter(avg_household_size > 1, avg_household_size < 5)

# Identify a few notable CBSAs to highlight
notable_cbsas <- c(
  "35620",  # New York
  "31080",  # Los Angeles
  "16980",  # Chicago
  "19100",  # Dallas
  "26420",  # Houston
  "37980",  # Philadelphia
  "47900",  # Washington DC
  "33100",  # Miami
  "12060",  # Atlanta
  "41860"   # San Francisco
)

# Create data set with highlighted CBSAs
household_size_plot <- household_size |>
  mutate(
    is_notable = GEOID %in% notable_cbsas,
    # Create shorter names for legend
    short_name = case_when(
      str_detect(NAME, "New York") ~ "New York",
      str_detect(NAME, "Los Angeles") ~ "Los Angeles",
      str_detect(NAME, "Chicago") ~ "Chicago",
      str_detect(NAME, "Dallas") ~ "Dallas",
      str_detect(NAME, "Houston") ~ "Houston",
      str_detect(NAME, "Philadelphia") ~ "Philadelphia",
      str_detect(NAME, "Washington") ~ "Washington DC",
      str_detect(NAME, "Miami") ~ "Miami",
      str_detect(NAME, "Atlanta") ~ "Atlanta",
      str_detect(NAME, "San Francisco") ~ "San Francisco",
      TRUE ~ "Other CBSAs"
    )
  )

# Create the visualization
ggplot(household_size_plot, aes(x = year, y = avg_household_size, group = GEOID)) +
  # Plot all CBSAs in gray with low alpha
  geom_line(
    data = household_size_plot |> filter(!is_notable),
    alpha = 0.1,
    color = "gray70",
    linewidth = 0.3
  ) +
  # Highlight notable CBSAs with colors
  geom_line(
    data = household_size_plot |> filter(is_notable),
    aes(color = short_name),
    linewidth = 1.2,
    alpha = 0.8
  ) +
  # Use distinct colors for each highlighted CBSA
  scale_color_brewer(palette = "Set3", name = "Major Metro Areas") +
  # Format y-axis
  scale_y_continuous(
    breaks = seq(2.0, 3.5, 0.25),
    limits = c(2.0, 3.5)
  ) +
  # Format x-axis
  scale_x_continuous(breaks = seq(2009, 2023, 2)) +
  # Add proper labels
  labs(
    title = "Evolution of Average Household Size Across US Metro Areas (2009-2023)",
    subtitle = "Gray lines show all CBSAs; colored lines highlight 10 largest metro areas",
    x = "Year",
    y = "Average Household Size (Persons per Household)",
    caption = "Data Source: American Community Survey (ACS)\nNote: 2020 data unavailable due to COVID-19 survey suspension"
  ) +
  # Clean theme
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "gray40"),
    axis.title = element_text(face = "bold"),
    legend.position = "right",
    legend.title = element_text(face = "bold", size = 10),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

# Calculate overall trend
overall_trend <- household_size |>
  group_by(year) |>
  summarize(national_avg = mean(avg_household_size, na.rm = TRUE))

start_size <- overall_trend$national_avg[1]
end_size <- overall_trend$national_avg[nrow(overall_trend)]
change_pct <- ((end_size - start_size) / start_size) * 100
```

**Observation:** Average household size has remained relatively stable throughout most US metro areas from 2009 to 2023, hovering around **2.5 to 2.7 persons per household**. The national average declined slightly from **`r round(start_size, 2)`** persons per household in 2009 to **`r round(end_size, 2)`** in 2023, a change of **`r round(change_pct, 1)`%**. 

The visualization reveals interesting patterns:
- Most CBSAs (shown in gray) follow similar trajectories, suggesting nationwide demographic trends.
- The gap in 2020 (due to COVID-19 survey suspension) is clear.
- Major metro areas (colored lines) show some variation, with some southwestern metros maintaining slightly larger household sizes.
- The overall downward trend reflects continued demographic shifts including delayed marriage, lower birth rates, and increasing numbers of single-person households.

This trend has important implications for housing policy: even as population grows, the number of households grows faster, increasing housing demand beyond what raw population growth would suggest.








Code related to Exploratory Data Analysis goes here. This may include
exploratory graphics, instructor-provided exploratory questions, or other
similar elements. 

## Final Insights and Deliverable

Code related to the final deliverable of the assignment goes here. 



------------------------------------------------------------------------

This work ©2025 by cguirand27 was initially prepared as a Mini-Project for
STA 9750 at Baruch College. More details about this course can be found at
[the course site](https://michael-weylandt.com/STA9750) and instructions for
this assignment can be found at 
[MP #02](https://michael-weylandt.com/STA9750/miniprojects/mini02.html)
