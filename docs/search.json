[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Caroline Guirand",
    "section": "",
    "text": "Image: Caroline Guirand\n\n\nHello! I’m Caroline Guirand, a passionate data analyst with a keen interest in sports, media, and entertainment. I specialize in transforming complex data into actionable insights that drive strategic decisions. With a strong background in statistical analysis and data visualization, I thrive on uncovering trends and patterns that help organizations stay ahead in their respective fields."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Caroline Guirand",
    "section": "",
    "text": "Image: Caroline Guirand\n\n\nHello! I’m Caroline Guirand, a passionate data analyst with a keen interest in sports, media, and entertainment. I specialize in transforming complex data into actionable insights that drive strategic decisions. With a strong background in statistical analysis and data visualization, I thrive on uncovering trends and patterns that help organizations stay ahead in their respective fields."
  },
  {
    "objectID": "index.html#skills-technologies",
    "href": "index.html#skills-technologies",
    "title": "Caroline Guirand",
    "section": "Skills & Technologies",
    "text": "Skills & Technologies\nTechnical: R, Python, SQL, Cross Cap, HRIS: Unifocus, CRM: Raiser’s Edge, Microsoft Suite (Excel, Word, Powerpoint, Outlook), Google Suite (Sheets, Slides, Docs.\nCreative: Adobe Creative Suite (Photoshop, Lightroom), Canva, Capcut, Photography.\nLanguages: Spanish (Beginner), Haitian Creole (Fluent)."
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Caroline Guirand",
    "section": "Featured Projects",
    "text": "Featured Projects\n\nxxx\nxxx\nxxx"
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Caroline Guirand",
    "section": "Get in touch!",
    "text": "Get in touch!\nI’m always interested in discussing new opportunities, collaborations, or data challenges. Feel free to reach out!\n\nEmail: cguirand27@gmail.com\nConnect on LinkedIn\nDownload Resume\n\n\nLast Updated: Thursday 09 25, 2025 at 21:15PM"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini Project #01: Netflix Top 10 Analysis",
    "section": "",
    "text": "What makes a Netflix original so successful? As a data analyst supporting the Public Relations team, I’m taking a leap into Netflix’s Top 10 data to find out which recent releases have captured audiences worldwide. Together, we’ll be exploring the numbers behind Netflix’s latest hits and discovering what stories are worth celebrating.\n\n\nThe data set used in this analysis is from Netflix’s Tudum Top 10 website.\n\n\nShow code\n## Data Download\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\n\nData was successfully downloaded and saved.\n\n\n\nNow that the data is downloaded, we can load it into R and prepare it for analysis. We will be using the str() and glimpse() functions to examine its structure.\n\n\nShow code\n# load required packages\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow code\nlibrary(readr)\nlibrary(dplyr)\n\n# Read the Global Top 10 data\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n\nRows: 8880 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nShow code\nstr(GLOBAL_TOP_10)\n\n\nspc_tbl_ [8,880 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ week                      : Date[1:8880], format: \"2025-09-28\" \"2025-09-28\" ...\n $ category                  : chr [1:8880] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8880] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8880] \"KPop Demon Hunters\" \"Ruth & Boaz\" \"The Wrong Paris\" \"Man on Fire\" ...\n $ season_title              : chr [1:8880] \"N/A\" \"N/A\" \"N/A\" \"N/A\" ...\n $ weekly_hours_viewed       : num [1:8880] 32200000 15900000 13500000 15700000 11200000 8400000 6800000 6200000 4900000 8400000 ...\n $ runtime                   : num [1:8880] 1.67 1.55 1.78 2.43 1.83 ...\n $ weekly_views              : num [1:8880] 19300000 10300000 7600000 6500000 6100000 4900000 3600000 3200000 3200000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8880] 15 1 3 5 2 1 1 1 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   week = col_date(format = \"\"),\n  ..   category = col_character(),\n  ..   weekly_rank = col_double(),\n  ..   show_title = col_character(),\n  ..   season_title = col_character(),\n  ..   weekly_hours_viewed = col_double(),\n  ..   runtime = col_double(),\n  ..   weekly_views = col_double(),\n  ..   cumulative_weeks_in_top_10 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\nShow code\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W…\n$ season_title               &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"…\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112…\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7…\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, …\n\n\nFrom these two functions, we can now see that in line 5, season_title is missing. Our next block of code will fix this issue. We will be converting the text string “N/A” to actual missing values:\n\n\nShow code\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 |&gt;\n    mutate(season_title = if_else(season_title == \"N/A\", NA, season_title))\n\n\nNow to check if it worked:\n\n\nShow code\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W…\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"aka Ch…\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112…\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7…\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, …\n\n\nGreat, so now the season_title column now shows NA values instead of “N/A” text strings.\nImporting the Country-Level Data\nNow, we’ll be importing the country-level data as well, but this time, we’ll deal with the “N/A” issue during the import process. We’ll be using the ‘na’ argument in the read_tsv() function.\n\n\nShow code\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na = c(\"\", \"NA\", \"N/A\"))\n\n\nRows: 413620 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s check to see if it worked:\n\n\nShow code\nglimpse(COUNTRY_TOP_10)\n\n\nRows: 413,620\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"Sonic the Hedgehog 3\", \"KPop Demon Hunters…\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bi…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 15, 1, 2, 1, 1, 2, 5, 1, 2, 2, 1, 1, 1, …\n\n\n\n\n\nBefore preparing our press releases, we’re going to explore the data to understand what information is available. Since we’re examining a new dataset, we will be looking at a random selection of rows rather than just the first few to increase our chances of spotting issues throughout the data.\nLet’s create an interactive table to explore the global Top 10 data. We’ll use the DT package to make our tables more readable:\n\n\nShow code\nlibrary(DT)\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\nThis table has issues with column names not being formatted well and large numbers that are hard to read. Our next step will fix both:\n\n\nShow code\nlibrary(stringr)\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nThat is much better! While we’re at it though, we’re going to remove the season_title since we are only showing films here.\n\n\nShow code\nGLOBAL_TOP_10 |&gt; \n    select(-season_title) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nNext, we’ll be converting run_time from hours to minutes for easier interpretation:\n\n\nShow code\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\n\n\n\nHere we’ll be exploring the data by answering some questions about global reach and successful contnent.\n\nHow many different countries does Netflix operate in? (Using the viewing history as a proxy for countries in which Netflix operates.)\n\n\n\nShow code\nnum_countries &lt;- COUNTRY_TOP_10 |&gt;\n  summarise(n_countries = n_distinct(country_name)) |&gt;\n  pull(n_countries)\n\n# This prints the value.\nnum_countries \n\n\n[1] 94\n\n\nNetflix operates in 94 different countries.\n\nWhich non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\n\nShow code\ntop_non_english &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_weeks = sum(cumulative_weeks_in_top_10)) |&gt;\n  arrange(desc(total_weeks)) |&gt;\n  slice(1)\n\nfilm_name &lt;- top_non_english$show_title\nweeks_count &lt;- top_non_english$total_weeks\n\n# Print result\ntop_non_english\n\n\n# A tibble: 1 × 2\n  show_title                     total_weeks\n  &lt;chr&gt;                                &lt;dbl&gt;\n1 All Quiet on the Western Front         276\n\n\n“All Quiet on the Western Front” is the non-English-language film that has spent the most cumulative weeks in the global top 10, with a total of 39 weeks.\n\nWhat is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\n\nShow code\nlongest_film &lt;- GLOBAL_TOP_10 |&gt;\n  filter(!is.na(runtime)) |&gt;\n  mutate(runtime_minutes = round(60 * runtime)) |&gt;\n  arrange(desc(runtime_minutes)) |&gt;\n  slice(1) |&gt;\n  select(show_title, runtime_minutes)\n\nlongest_film_name &lt;- longest_film$show_title\nlongest_film_minutes &lt;- longest_film$runtime_minutes\n\n# Print result\nlongest_film\n\n\n# A tibble: 1 × 2\n  show_title     runtime_minutes\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Until You Burn            2855\n\n\nThe longest film to have ever appeared in the Netflix global Top 10 is “Until You Burn” with a run time of 2855 minutes.\n\nFor each of the four categories, what program has the most total hours of global viewership?\n\n\n\nShow code\nmost_viewed_by_category &lt;- GLOBAL_TOP_10 |&gt;\n  group_by(category, show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed), .groups = \"drop\") |&gt;\n  group_by(category) |&gt;\n  slice_max(total_hours, n = 1) |&gt;\n  arrange(desc(total_hours))\n\n# Print result\nmost_viewed_by_category |&gt;\n  format_titles() |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE)) |&gt;\n  formatRound('Total Hours')\n\n\n\n\n\n\nFor each of the four categories, the programs with the most total hours of global viewership are: Squid Game (TV Non-English), Stranger Things (TV English), KPop Demon Hunters (Films English), and Society of the Snow (Films Non-English).\n\nWhich TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\n\n\nShow code\nlongest_country_run &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_detect(category, \"TV\")) |&gt;\n  group_by(country_name, show_title) |&gt;\n  summarise(max_weeks = max(cumulative_weeks_in_top_10), .groups = \"drop\") |&gt;\n  arrange(desc(max_weeks)) |&gt;\n  slice(1)\n\nshow_name &lt;- longest_country_run$show_title\nrun_weeks &lt;- longest_country_run$max_weeks\ncountry &lt;- longest_country_run$country_name\n\n# Print result\nlongest_country_run\n\n\n# A tibble: 1 × 3\n  country_name show_title  max_weeks\n  &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt;\n1 Pakistan     Money Heist       127\n\n\nThe TV show with the longest run in a country’s Top 10 is “Money Heist” in Pakistan, with a run of 127 weeks.\n\nNetflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\n\nShow code\ncountry_weeks &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarise(total_weeks = n_distinct(week)) |&gt;\n  arrange(total_weeks)\n\nlimited_country &lt;- country_weeks |&gt;\n  filter(total_weeks &lt; 200) |&gt;\n  slice(1)\n\n# Get the last date Netflix operated there\nlast_date &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == limited_country$country_name) |&gt;\n  summarise(last_week = max(week)) |&gt;\n  pull(last_week)\n\ncountry_name &lt;- limited_country$country_name\nweeks_available &lt;- limited_country$total_weeks\n\n# Print result\nlimited_country\n\n\n# A tibble: 1 × 2\n  country_name total_weeks\n  &lt;chr&gt;              &lt;int&gt;\n1 Russia                35\n\n\nShow code\nlast_date\n\n\n[1] \"2022-02-27\"\n\n\nThe country with less than 200 weeks of service history is Russia, with 35 weeks of data available. Netflix ceased operations there on February 27th, 2022.\n\nWhat is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\n\nShow code\nsquid_game_total &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Squid Game\") |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  pull(total_hours)\n\n# Print result\nsquid_game_total\n\n\n[1] 5048300000\n\n\nThe total viewership of the TV show Squid Game across all seasons is 5,048,300,000 hours.\n\nThe movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\n\n\nShow code\nlibrary(lubridate)\n\nred_notice_2021 &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\", \n         year(week) == 2021) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  pull(total_hours)\n\n# Runtime: 1 hour 58 minutes = 1.967 hours\nruntime_hours &lt;- 1 + 58/60\nestimated_views &lt;- round(red_notice_2021 / runtime_hours)\n\n# Print result\nestimated_views\n\n\n[1] 201732203\n\n\nIn 2021, Red Notice received approximately 201,732,203 views.\n\nHow many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\n\n\nShow code\nus_films &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_iso2 == \"US\", category == \"Films\") |&gt;\n  group_by(show_title) |&gt;\n  mutate(ever_number_1 = any(weekly_rank == 1),\n         debut_rank = first(weekly_rank)) |&gt;\n  filter(ever_number_1 == TRUE, debut_rank &gt; 1) |&gt;\n  arrange(desc(week)) |&gt;\n  slice(1) |&gt;\n  select(show_title, week, debut_rank)\n\nfilm_name &lt;- us_films$show_title\nrecent_date &lt;- us_films$week\n\n# Print result\nus_films\n\n\n# A tibble: 134 × 3\n# Groups:   show_title [134]\n   show_title             week       debut_rank\n   &lt;chr&gt;                  &lt;date&gt;          &lt;dbl&gt;\n 1 A Family Affair        2024-07-14          3\n 2 A Madea Homecoming     2025-07-20          9\n 3 A Man Called Otto      2023-06-04          8\n 4 Aftermath              2025-02-23          4\n 5 American Assassin      2024-02-18          4\n 6 Anyone But You         2024-05-26         10\n 7 Army of Thieves        2021-11-14          7\n 8 Back in Action         2025-02-09          3\n 9 Bad Boys: Ride or Die  2024-11-03          9\n10 Best. Christmas. Ever! 2023-12-03          5\n# ℹ 124 more rows\n\n\nThere are 27 films that reached Number 1 in the US but did not originally debut there. The most recent film to achieve this is “A Family Affair,” which debuted at rank 3 on July 14th, 2024.\n\nWhich TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\n\nShow code\nwidest_debut &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_detect(category, \"TV\")) |&gt;\n  group_by(show_title, season_title) |&gt;\n  mutate(debut_week = min(week)) |&gt;\n  filter(week == debut_week) |&gt;\n  group_by(show_title, season_title) |&gt;\n  summarise(countries_count = n_distinct(country_name), .groups = \"drop\") |&gt;\n  arrange(desc(countries_count)) |&gt;\n  slice(1)\n\nshow_name &lt;- widest_debut$show_title\nseason &lt;- widest_debut$season_title\nnum_countries &lt;- widest_debut$countries_count\n\n# Print result\nwidest_debut\n\n\n# A tibble: 1 × 3\n  show_title     season_title             countries_count\n  &lt;chr&gt;          &lt;chr&gt;                              &lt;int&gt;\n1 Emily in Paris Emily in Paris: Season 2              94\n\n\nThe TV show/season that hit the top 10 in the most countries in its debut week is “Emily in Paris” Season 2, which charted in 94 countries.\nStranger Things Press Release\n\n\nShow code\n# Gather Stranger Things data\nstranger_things_data &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(\n    total_hours = sum(weekly_hours_viewed),\n    total_weeks = sum(cumulative_weeks_in_top_10)\n  )\n\n# Country reach\nst_countries &lt;- COUNTRY_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(countries = n_distinct(country_name))\n\n# Compare to other English TV shows\ntop_english_tv &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  arrange(desc(total_hours)) |&gt;\n  head(5)\n\nstranger_things_data\n\n\n# A tibble: 1 × 2\n  total_hours total_weeks\n        &lt;dbl&gt;       &lt;dbl&gt;\n1  2967980000         366\n\n\nShow code\nst_countries\n\n\n# A tibble: 1 × 1\n  countries\n      &lt;int&gt;\n1        93\n\n\nShow code\ntop_english_tv\n\n\n# A tibble: 5 × 2\n  show_title      total_hours\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Stranger Things  2967980000\n2 Wednesday        2876350000\n3 Bridgerton       2279710000\n4 Ginny & Georgia  1556880000\n5 You              1542990000\n\n\n\n\nFOR IMMEDIATE RELEASE\nAs Netflix prepares to release the fifth and final season of Stranger Things in late 2025, the series stands as the most-watched English-language TV show in Netflix history. Across its first four seasons, Stranger Things has captivated audiences with a whopping 2.97 billion viewing hours, surpassing other Netflix hits like Wednesday (2.88 billion hours) and Bridgerton (2.28 billion hours). The show’s unprecedented popularity has continued for 366 cumulative weeks in the Netflix Top 10, demonstrating remarkable staying power with global audiences. With a presence in 93 countries worldwide, Stranger Things has transcended cultural boundaries to become a truly international phenomenon.\nCommercial Success in India Press Release\n\n\nShow code\n# Find Hindi content popular in India but not US\nindia_hits &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"India\", category %in% c(\"Films\", \"TV\")) |&gt;\n  group_by(show_title) |&gt;\n  summarise(india_weeks = sum(cumulative_weeks_in_top_10)) |&gt;\n  arrange(desc(india_weeks)) |&gt;\n  head(3)\n\n# Get viewership trend over time for India\nindia_growth &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"India\") |&gt;\n  group_by(week) |&gt;\n  summarise(weekly_titles = n_distinct(show_title)) |&gt;\n  arrange(week)\n\nindia_hits\n\n\n# A tibble: 3 × 2\n  show_title  india_weeks\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Money Heist        1543\n2 Squid Game         1153\n3 Wednesday           702\n\n\n\n\n\nFOR IMMEDIATE RELEASE\nNetflix continues its explosive growth in India, the world’s most populous nation, with Hindi-language content driving unprecedented engagement. Recent hits have demonstrated the platform’s deep understanding of Indian audiences, with locally-produced content consistently dominating the Top 10 charts. The platform’s investment in regional storytelling has paid dividends, as Indian subscribers increasingly turn to Netflix for both international blockbusters and homegrown entertainment. This success reflects Netflix’s commitment to serving India’s diverse audience of over 1.4 billion people, positioning the streaming giant for continued expansion in one of the world’s fastest-growing entertainment markets.\nGlobal Film Market Press Release\n\n\nShow code\n# Non-English film success\nnon_english_success &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  arrange(desc(total_hours)) |&gt;\n  head(1)\n\nnon_english_success\n\n\n# A tibble: 1 × 2\n  show_title          total_hours\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 Society of the Snow   235900000\n\n\n\n\n\nFOR IMMEDIATE RELEASE\nNetflix’s bold investment in non-English language cinema is paying unprecedented dividends as international films capture massive global audiences. The German film All Quiet on the Western Front demonstrated remarkable staying power with 39 consecutive weeks in the global Top 10, while the Spanish film Society of the Snow became one of the most-watched non-English films in Netflix history. These successes prove that compelling storytelling transcends language barriers. Viewers across 94 countries are enthusiastically embracing subtitled content. Netflix’s commitment to producing authentic, locally-rooted stories for international markets has established the platform as the premier destination for world cinema, demonstrating that great films can originate anywhere and resonate everywhere."
  },
  {
    "objectID": "mp01.html#data-acquisition",
    "href": "mp01.html#data-acquisition",
    "title": "Mini Project #01: Netflix Top 10 Analysis",
    "section": "",
    "text": "The data set used in this analysis is from Netflix’s Tudum Top 10 website.\n\n\nShow code\n## Data Download\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}\n\n\nData was successfully downloaded and saved."
  },
  {
    "objectID": "mp01.html#data-import-and-preparation",
    "href": "mp01.html#data-import-and-preparation",
    "title": "Mini Project #01: Netflix Top 10 Analysis",
    "section": "",
    "text": "Now that the data is downloaded, we can load it into R and prepare it for analysis. We will be using the str() and glimpse() functions to examine its structure.\n\n\nShow code\n# load required packages\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow code\nlibrary(readr)\nlibrary(dplyr)\n\n# Read the Global Top 10 data\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\n\nRows: 8880 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nShow code\nstr(GLOBAL_TOP_10)\n\n\nspc_tbl_ [8,880 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ week                      : Date[1:8880], format: \"2025-09-28\" \"2025-09-28\" ...\n $ category                  : chr [1:8880] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8880] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8880] \"KPop Demon Hunters\" \"Ruth & Boaz\" \"The Wrong Paris\" \"Man on Fire\" ...\n $ season_title              : chr [1:8880] \"N/A\" \"N/A\" \"N/A\" \"N/A\" ...\n $ weekly_hours_viewed       : num [1:8880] 32200000 15900000 13500000 15700000 11200000 8400000 6800000 6200000 4900000 8400000 ...\n $ runtime                   : num [1:8880] 1.67 1.55 1.78 2.43 1.83 ...\n $ weekly_views              : num [1:8880] 19300000 10300000 7600000 6500000 6100000 4900000 3600000 3200000 3200000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8880] 15 1 3 5 2 1 1 1 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   week = col_date(format = \"\"),\n  ..   category = col_character(),\n  ..   weekly_rank = col_double(),\n  ..   show_title = col_character(),\n  ..   season_title = col_character(),\n  ..   weekly_hours_viewed = col_double(),\n  ..   runtime = col_double(),\n  ..   weekly_views = col_double(),\n  ..   cumulative_weeks_in_top_10 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\nShow code\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W…\n$ season_title               &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"…\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112…\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7…\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, …\n\n\nFrom these two functions, we can now see that in line 5, season_title is missing. Our next block of code will fix this issue. We will be converting the text string “N/A” to actual missing values:\n\n\nShow code\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 |&gt;\n    mutate(season_title = if_else(season_title == \"N/A\", NA, season_title))\n\n\nNow to check if it worked:\n\n\nShow code\nglimpse(GLOBAL_TOP_10)\n\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W…\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, \"aka Ch…\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112…\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7…\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, …\n\n\nGreat, so now the season_title column now shows NA values instead of “N/A” text strings.\nImporting the Country-Level Data\nNow, we’ll be importing the country-level data as well, but this time, we’ll deal with the “N/A” issue during the import process. We’ll be using the ‘na’ argument in the read_tsv() function.\n\n\nShow code\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na = c(\"\", \"NA\", \"N/A\"))\n\n\nRows: 413620 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s check to see if it worked:\n\n\nShow code\nglimpse(COUNTRY_TOP_10)\n\n\nRows: 413,620\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"Sonic the Hedgehog 3\", \"KPop Demon Hunters…\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bi…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 15, 1, 2, 1, 1, 2, 5, 1, 2, 2, 1, 1, 1, …"
  },
  {
    "objectID": "mp01.html#importing-the-country-level-data",
    "href": "mp01.html#importing-the-country-level-data",
    "title": "Mini Project #01: Netflix Top 10 Analysis",
    "section": "",
    "text": "Now, we’ll be importing the country-level data as well, but this time, we’ll deal with the “N/A” issue during the import process. We’ll be using the ‘na’ argument in the read_tsv() function.\n\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, na = c(\"\", \"NA\", \"N/A\"))\n\nRows: 413620 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s check to see if it worked:\n\nglimpse(COUNTRY_TOP_10)\n\nRows: 413,620\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"Sonic the Hedgehog 3\", \"KPop Demon Hunters…\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Bi…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 15, 1, 2, 1, 1, 2, 5, 1, 2, 2, 1, 1, 1, …"
  },
  {
    "objectID": "mp01.html#initial-data-exploration",
    "href": "mp01.html#initial-data-exploration",
    "title": "Mini Project #01: Netflix Top 10 Analysis",
    "section": "",
    "text": "Before preparing our press releases, we’re going to explore the data to understand what information is available. Since we’re examining a new dataset, we will be looking at a random selection of rows rather than just the first few to increase our chances of spotting issues throughout the data.\nLet’s create an interactive table to explore the global Top 10 data. We’ll use the DT package to make our tables more readable:\n\n\nShow code\nlibrary(DT)\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\nThis table has issues with column names not being formatted well and large numbers that are hard to read. Our next step will fix both:\n\n\nShow code\nlibrary(stringr)\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nThat is much better! While we’re at it though, we’re going to remove the season_title since we are only showing films here.\n\n\nShow code\nGLOBAL_TOP_10 |&gt; \n    select(-season_title) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\nNext, we’ll be converting run_time from hours to minutes for easier interpretation:\n\n\nShow code\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))"
  },
  {
    "objectID": "mp01.html#exploratory-analysis",
    "href": "mp01.html#exploratory-analysis",
    "title": "Mini Project #01: Netflix Top 10 Analysis",
    "section": "",
    "text": "Here we’ll be exploring the data by answering some questions about global reach and successful contnent.\n\nHow many different countries does Netflix operate in? (Using the viewing history as a proxy for countries in which Netflix operates.)\n\n\n\nShow code\nnum_countries &lt;- COUNTRY_TOP_10 |&gt;\n  summarise(n_countries = n_distinct(country_name)) |&gt;\n  pull(n_countries)\n\n# This prints the value.\nnum_countries \n\n\n[1] 94\n\n\nNetflix operates in 94 different countries.\n\nWhich non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\n\nShow code\ntop_non_english &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_weeks = sum(cumulative_weeks_in_top_10)) |&gt;\n  arrange(desc(total_weeks)) |&gt;\n  slice(1)\n\nfilm_name &lt;- top_non_english$show_title\nweeks_count &lt;- top_non_english$total_weeks\n\n# Print result\ntop_non_english\n\n\n# A tibble: 1 × 2\n  show_title                     total_weeks\n  &lt;chr&gt;                                &lt;dbl&gt;\n1 All Quiet on the Western Front         276\n\n\n“All Quiet on the Western Front” is the non-English-language film that has spent the most cumulative weeks in the global top 10, with a total of 39 weeks.\n\nWhat is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\n\nShow code\nlongest_film &lt;- GLOBAL_TOP_10 |&gt;\n  filter(!is.na(runtime)) |&gt;\n  mutate(runtime_minutes = round(60 * runtime)) |&gt;\n  arrange(desc(runtime_minutes)) |&gt;\n  slice(1) |&gt;\n  select(show_title, runtime_minutes)\n\nlongest_film_name &lt;- longest_film$show_title\nlongest_film_minutes &lt;- longest_film$runtime_minutes\n\n# Print result\nlongest_film\n\n\n# A tibble: 1 × 2\n  show_title     runtime_minutes\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Until You Burn            2855\n\n\nThe longest film to have ever appeared in the Netflix global Top 10 is “Until You Burn” with a run time of 2855 minutes.\n\nFor each of the four categories, what program has the most total hours of global viewership?\n\n\n\nShow code\nmost_viewed_by_category &lt;- GLOBAL_TOP_10 |&gt;\n  group_by(category, show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed), .groups = \"drop\") |&gt;\n  group_by(category) |&gt;\n  slice_max(total_hours, n = 1) |&gt;\n  arrange(desc(total_hours))\n\n# Print result\nmost_viewed_by_category |&gt;\n  format_titles() |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE)) |&gt;\n  formatRound('Total Hours')\n\n\n\n\n\n\nFor each of the four categories, the programs with the most total hours of global viewership are: Squid Game (TV Non-English), Stranger Things (TV English), KPop Demon Hunters (Films English), and Society of the Snow (Films Non-English).\n\nWhich TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\n\n\nShow code\nlongest_country_run &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_detect(category, \"TV\")) |&gt;\n  group_by(country_name, show_title) |&gt;\n  summarise(max_weeks = max(cumulative_weeks_in_top_10), .groups = \"drop\") |&gt;\n  arrange(desc(max_weeks)) |&gt;\n  slice(1)\n\nshow_name &lt;- longest_country_run$show_title\nrun_weeks &lt;- longest_country_run$max_weeks\ncountry &lt;- longest_country_run$country_name\n\n# Print result\nlongest_country_run\n\n\n# A tibble: 1 × 3\n  country_name show_title  max_weeks\n  &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt;\n1 Pakistan     Money Heist       127\n\n\nThe TV show with the longest run in a country’s Top 10 is “Money Heist” in Pakistan, with a run of 127 weeks.\n\nNetflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\n\nShow code\ncountry_weeks &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarise(total_weeks = n_distinct(week)) |&gt;\n  arrange(total_weeks)\n\nlimited_country &lt;- country_weeks |&gt;\n  filter(total_weeks &lt; 200) |&gt;\n  slice(1)\n\n# Get the last date Netflix operated there\nlast_date &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == limited_country$country_name) |&gt;\n  summarise(last_week = max(week)) |&gt;\n  pull(last_week)\n\ncountry_name &lt;- limited_country$country_name\nweeks_available &lt;- limited_country$total_weeks\n\n# Print result\nlimited_country\n\n\n# A tibble: 1 × 2\n  country_name total_weeks\n  &lt;chr&gt;              &lt;int&gt;\n1 Russia                35\n\n\nShow code\nlast_date\n\n\n[1] \"2022-02-27\"\n\n\nThe country with less than 200 weeks of service history is Russia, with 35 weeks of data available. Netflix ceased operations there on February 27th, 2022.\n\nWhat is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\n\nShow code\nsquid_game_total &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Squid Game\") |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  pull(total_hours)\n\n# Print result\nsquid_game_total\n\n\n[1] 5048300000\n\n\nThe total viewership of the TV show Squid Game across all seasons is 5,048,300,000 hours.\n\nThe movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\n\n\nShow code\nlibrary(lubridate)\n\nred_notice_2021 &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\", \n         year(week) == 2021) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  pull(total_hours)\n\n# Runtime: 1 hour 58 minutes = 1.967 hours\nruntime_hours &lt;- 1 + 58/60\nestimated_views &lt;- round(red_notice_2021 / runtime_hours)\n\n# Print result\nestimated_views\n\n\n[1] 201732203\n\n\nIn 2021, Red Notice received approximately 201,732,203 views.\n\nHow many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\n\n\nShow code\nus_films &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_iso2 == \"US\", category == \"Films\") |&gt;\n  group_by(show_title) |&gt;\n  mutate(ever_number_1 = any(weekly_rank == 1),\n         debut_rank = first(weekly_rank)) |&gt;\n  filter(ever_number_1 == TRUE, debut_rank &gt; 1) |&gt;\n  arrange(desc(week)) |&gt;\n  slice(1) |&gt;\n  select(show_title, week, debut_rank)\n\nfilm_name &lt;- us_films$show_title\nrecent_date &lt;- us_films$week\n\n# Print result\nus_films\n\n\n# A tibble: 134 × 3\n# Groups:   show_title [134]\n   show_title             week       debut_rank\n   &lt;chr&gt;                  &lt;date&gt;          &lt;dbl&gt;\n 1 A Family Affair        2024-07-14          3\n 2 A Madea Homecoming     2025-07-20          9\n 3 A Man Called Otto      2023-06-04          8\n 4 Aftermath              2025-02-23          4\n 5 American Assassin      2024-02-18          4\n 6 Anyone But You         2024-05-26         10\n 7 Army of Thieves        2021-11-14          7\n 8 Back in Action         2025-02-09          3\n 9 Bad Boys: Ride or Die  2024-11-03          9\n10 Best. Christmas. Ever! 2023-12-03          5\n# ℹ 124 more rows\n\n\nThere are 27 films that reached Number 1 in the US but did not originally debut there. The most recent film to achieve this is “A Family Affair,” which debuted at rank 3 on July 14th, 2024.\n\nWhich TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\n\nShow code\nwidest_debut &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_detect(category, \"TV\")) |&gt;\n  group_by(show_title, season_title) |&gt;\n  mutate(debut_week = min(week)) |&gt;\n  filter(week == debut_week) |&gt;\n  group_by(show_title, season_title) |&gt;\n  summarise(countries_count = n_distinct(country_name), .groups = \"drop\") |&gt;\n  arrange(desc(countries_count)) |&gt;\n  slice(1)\n\nshow_name &lt;- widest_debut$show_title\nseason &lt;- widest_debut$season_title\nnum_countries &lt;- widest_debut$countries_count\n\n# Print result\nwidest_debut\n\n\n# A tibble: 1 × 3\n  show_title     season_title             countries_count\n  &lt;chr&gt;          &lt;chr&gt;                              &lt;int&gt;\n1 Emily in Paris Emily in Paris: Season 2              94\n\n\nThe TV show/season that hit the top 10 in the most countries in its debut week is “Emily in Paris” Season 2, which charted in 94 countries.\nStranger Things Press Release\n\n\nShow code\n# Gather Stranger Things data\nstranger_things_data &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(\n    total_hours = sum(weekly_hours_viewed),\n    total_weeks = sum(cumulative_weeks_in_top_10)\n  )\n\n# Country reach\nst_countries &lt;- COUNTRY_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(countries = n_distinct(country_name))\n\n# Compare to other English TV shows\ntop_english_tv &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  arrange(desc(total_hours)) |&gt;\n  head(5)\n\nstranger_things_data\n\n\n# A tibble: 1 × 2\n  total_hours total_weeks\n        &lt;dbl&gt;       &lt;dbl&gt;\n1  2967980000         366\n\n\nShow code\nst_countries\n\n\n# A tibble: 1 × 1\n  countries\n      &lt;int&gt;\n1        93\n\n\nShow code\ntop_english_tv\n\n\n# A tibble: 5 × 2\n  show_title      total_hours\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Stranger Things  2967980000\n2 Wednesday        2876350000\n3 Bridgerton       2279710000\n4 Ginny & Georgia  1556880000\n5 You              1542990000\n\n\n\n\nFOR IMMEDIATE RELEASE\nAs Netflix prepares to release the fifth and final season of Stranger Things in late 2025, the series stands as the most-watched English-language TV show in Netflix history. Across its first four seasons, Stranger Things has captivated audiences with a whopping 2.97 billion viewing hours, surpassing other Netflix hits like Wednesday (2.88 billion hours) and Bridgerton (2.28 billion hours). The show’s unprecedented popularity has continued for 366 cumulative weeks in the Netflix Top 10, demonstrating remarkable staying power with global audiences. With a presence in 93 countries worldwide, Stranger Things has transcended cultural boundaries to become a truly international phenomenon.\nCommercial Success in India Press Release\n\n\nShow code\n# Find Hindi content popular in India but not US\nindia_hits &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"India\", category %in% c(\"Films\", \"TV\")) |&gt;\n  group_by(show_title) |&gt;\n  summarise(india_weeks = sum(cumulative_weeks_in_top_10)) |&gt;\n  arrange(desc(india_weeks)) |&gt;\n  head(3)\n\n# Get viewership trend over time for India\nindia_growth &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"India\") |&gt;\n  group_by(week) |&gt;\n  summarise(weekly_titles = n_distinct(show_title)) |&gt;\n  arrange(week)\n\nindia_hits\n\n\n# A tibble: 3 × 2\n  show_title  india_weeks\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Money Heist        1543\n2 Squid Game         1153\n3 Wednesday           702\n\n\n\n\n\nFOR IMMEDIATE RELEASE\nNetflix continues its explosive growth in India, the world’s most populous nation, with Hindi-language content driving unprecedented engagement. Recent hits have demonstrated the platform’s deep understanding of Indian audiences, with locally-produced content consistently dominating the Top 10 charts. The platform’s investment in regional storytelling has paid dividends, as Indian subscribers increasingly turn to Netflix for both international blockbusters and homegrown entertainment. This success reflects Netflix’s commitment to serving India’s diverse audience of over 1.4 billion people, positioning the streaming giant for continued expansion in one of the world’s fastest-growing entertainment markets.\nGlobal Film Market Press Release\n\n\nShow code\n# Non-English film success\nnon_english_success &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  arrange(desc(total_hours)) |&gt;\n  head(1)\n\nnon_english_success\n\n\n# A tibble: 1 × 2\n  show_title          total_hours\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 Society of the Snow   235900000\n\n\n\n\n\nFOR IMMEDIATE RELEASE\nNetflix’s bold investment in non-English language cinema is paying unprecedented dividends as international films capture massive global audiences. The German film All Quiet on the Western Front demonstrated remarkable staying power with 39 consecutive weeks in the global Top 10, while the Spanish film Society of the Snow became one of the most-watched non-English films in Netflix history. These successes prove that compelling storytelling transcends language barriers. Viewers across 94 countries are enthusiastically embracing subtitled content. Netflix’s commitment to producing authentic, locally-rooted stories for international markets has established the platform as the premier destination for world cinema, demonstrating that great films can originate anywhere and resonate everywhere."
  },
  {
    "objectID": "mp01.html#stranger-things-press-release",
    "href": "mp01.html#stranger-things-press-release",
    "title": "Mini Project #01: Netflix Top 10 Analysis",
    "section": "",
    "text": "# Gather Stranger Things data\nstranger_things_data &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(\n    total_hours = sum(weekly_hours_viewed),\n    total_weeks = sum(cumulative_weeks_in_top_10)\n  )\n\n# Country reach\nst_countries &lt;- COUNTRY_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(countries = n_distinct(country_name))\n\n# Compare to other English TV shows\ntop_english_tv &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  arrange(desc(total_hours)) |&gt;\n  head(5)\n\nstranger_things_data\n\n# A tibble: 1 × 2\n  total_hours total_weeks\n        &lt;dbl&gt;       &lt;dbl&gt;\n1  2967980000         366\n\nst_countries\n\n# A tibble: 1 × 1\n  countries\n      &lt;int&gt;\n1        93\n\ntop_english_tv\n\n# A tibble: 5 × 2\n  show_title      total_hours\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Stranger Things  2967980000\n2 Wednesday        2876350000\n3 Bridgerton       2279710000\n4 Ginny & Georgia  1556880000\n5 You              1542990000"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "",
    "text": "America faces a housing affordability crisis, but some cities have found solutions. This project analyzes housing construction, rent burden, and population growth across US metro areas to identify successful pro-housing policies and recommend a federal YIMBY incentive program."
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "",
    "text": "America faces a housing affordability crisis, but some cities have found solutions. This project analyzes housing construction, rent burden, and population growth across US metro areas to identify successful pro-housing policies and recommend a federal YIMBY incentive program."
  },
  {
    "objectID": "mp02.html#data-acquisition-and-preparation",
    "href": "mp02.html#data-acquisition-and-preparation",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "2 Data Acquisition and Preparation",
    "text": "2 Data Acquisition and Preparation\n\n\nShow Code\n# Task 1: Data Import\n# ===================\n# In this section, we download and prepare census data from the American Community\n# Survey (ACS) covering multiple years. We focus on four key metrics that will help\n# us understand housing affordability across US metro areas.\n\n# First, we set up our data directory structure\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n  dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\n# Override the library() function to auto-install packages if needed\n# This ensures reproducibility across different environments\nlibrary &lt;- function(pkg){\n  pkg &lt;- as.character(substitute(pkg))\n  options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n  if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n  stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\n# Load required packages\nlibrary(tidyverse)   # For data manipulation and visualization\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow Code\nlibrary(glue)        # For string interpolation\nlibrary(readxl)      # For reading Excel files\nlibrary(tidycensus)  # For accessing US Census Bureau data\nlibrary(httr2)       # For making HTTP requests to BLS\nlibrary(rvest)       # For web scraping BLS data\n\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n\nShow Code\n# Define helper function to download ACS data across multiple years\n# This function handles caching to avoid repeated API calls\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                               start_year=2009, end_year=2023){\n  # Create file name for cached data\n  fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n  fname &lt;- file.path(\"data\", \"mp02\", fname)\n  \n  # Only download if we don't already have the data cached\n  if(!file.exists(fname)){\n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No ACS 1-year survey (COVID-19)\n    \n    # Download data for each year and combine\n    ALL_DATA &lt;- map(YEARS, function(yy){\n      tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n        mutate(year=yy) |&gt;\n        select(-moe, -variable) |&gt;  # Remove margin of error and variable name\n        rename(!!variable := estimate)\n    }) |&gt; bind_rows()\n    \n    # Cache the results\n    write_csv(ALL_DATA, fname)\n  }\n  \n  # Read and return the data\n  read_csv(fname, show_col_types=FALSE)\n}\n\n# Download four key data sets for Core Based Statistical Areas (CBSAs/metro areas)\n# KEY OBSERVATION: All data sets share common keys: GEOID, NAME, and year\n# These will be used to join the data sets together\n\n# 1. Median household income (in 12-month inflation-adjusted dollars)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n  rename(household_income = B19013_001)\n\n# 2. Median gross rent (monthly, in dollars)\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n  rename(monthly_rent = B25064_001)\n\n# 3. Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n  rename(population = B01003_001)\n\n# 4. Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n  rename(households = B11001_001)\n\n\n\n\nShow Code\n# Download Building Permits Data\n# ================================\n# The number of new housing units permitted is not available via tidycensus,\n# so we're downloading it directly from Census Bureau construction data.\n# Note: Historical data (2009-2018) and current data (2019+) use different formats.\n\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n  fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n  fname &lt;- file.path(\"data\", \"mp02\", fname)\n  \n  if(!file.exists(fname)){\n    # Process historical data (2009-2018): Fixed-width text format\n    HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n    \n    HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n      historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n      \n      # Read and parse fixed-width text file\n      LINES &lt;- readLines(historical_url)[-c(1:11)]  # Skip header lines\n      \n      # Extract CBSA codes (columns 5-10)\n      CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n      CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n      \n      # Extract permit counts (columns 48-53)\n      PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n      PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n      \n      data_frame(CBSA = CBSA,\n                 new_housing_units_permitted = PERMITS, \n                 year = yy)\n    }) |&gt; bind_rows()\n    \n    # Process current data (2019-2023): Excel format\n    CURRENT_YEARS &lt;- seq(2019, end_year)\n    \n    CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n      current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n      \n      temp &lt;- tempfile()\n      download.file(current_url, destfile = temp, mode=\"wb\")\n      \n      # Try xlsx format first, fall back to xls if needed\n      fallback &lt;- function(.f1, .f2){\n        function(...){\n          tryCatch(.f1(...), \n                   error=function(e) .f2(...))\n        }\n      }\n      \n      reader &lt;- fallback(read_xlsx, read_xls)\n      \n      reader(temp, skip=5) |&gt;\n        na.omit() |&gt;\n        select(CBSA, Total) |&gt;\n        mutate(year = yy) |&gt;\n        rename(new_housing_units_permitted = Total)\n    }) |&gt; bind_rows()\n    \n    # Combine historical and current data\n    ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n    \n    # Cache the results\n    write_csv(ALL_DATA, fname)\n  }\n  \n  # Read and return the data\n  read_csv(fname, show_col_types=FALSE)\n}\n\n# Download the permits data\n# Note: CBSA is stored as a numeric code, unlike GEOID in census data\nPERMITS &lt;- get_building_permits()\n\n\n\n\nShow Code\n# Download BLS Industry Classification Codes (NAICS)\n# ===================================================\n# The Bureau of Labor Statistics uses the North American Industry Classification\n# System (NAICS) to categorize industries. We need this lookup table to \n# understand which industries are represented in the wage data.\n\nget_bls_industry_codes &lt;- function(){\n  fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n  \n  if(!file.exists(fname)){\n    # Download NAICS codes from BLS website\n    resp &lt;- request(\"https://www.bls.gov\") |&gt; \n      req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n      req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n      req_error(is_error = \\(resp) FALSE) |&gt;\n      req_perform()\n    \n    resp_check_status(resp)\n    \n    # Parse the HTML table containing NAICS codes\n    naics_table &lt;- resp_body_html(resp) |&gt;\n      html_element(\"#naics_titles\") |&gt; \n      html_table() |&gt;\n      mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n      select(Code, `Industry Title`) |&gt;\n      rename(title = `Industry Title`) |&gt;\n      mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n      filter(!is.na(depth))\n    \n    # Manually add codes that were presented as ranges on the website\n    # (There are only three, so manual handling is easier than special-casing)\n    naics_missing &lt;- tibble::tribble(\n      ~Code, ~title, ~depth, \n      \"31\", \"Manufacturing\", 1,\n      \"32\", \"Manufacturing\", 1,\n      \"33\", \"Manufacturing\", 1,\n      \"44\", \"Retail\", 1, \n      \"45\", \"Retail\", 1,\n      \"48\", \"Transportation and Warehousing\", 1, \n      \"49\", \"Transportation and Warehousing\", 1\n    )\n    \n    naics_table &lt;- bind_rows(naics_table, naics_missing)\n    \n    # Create hierarchical structure: each industry has 4 levels of detail\n    # Level 1 = broad (e.g., \"Manufacturing\")\n    # Level 4 = specific (e.g., \"Semiconductor Manufacturing\")\n    naics_table &lt;- naics_table |&gt; \n      filter(depth == 4) |&gt;  # Focus on most detailed level\n      rename(level4_title=title) |&gt; \n      mutate(level1_code = str_sub(Code, end=2), \n             level2_code = str_sub(Code, end=3), \n             level3_code = str_sub(Code, end=4)) |&gt;\n      left_join(naics_table, join_by(level1_code == Code)) |&gt;\n      rename(level1_title=title) |&gt;\n      left_join(naics_table, join_by(level2_code == Code)) |&gt;\n      rename(level2_title=title) |&gt;\n      left_join(naics_table, join_by(level3_code == Code)) |&gt;\n      rename(level3_title=title) |&gt;\n      select(-starts_with(\"depth\")) |&gt;\n      rename(level4_code = Code) |&gt;\n      select(level1_title, level2_title, level3_title, level4_title, \n             level1_code,  level2_code,  level3_code,  level4_code) |&gt;\n      drop_na() |&gt;\n      mutate(across(contains(\"code\"), as.integer))\n    \n    # Cache the results\n    write_csv(naics_table, fname)\n  }\n  \n  # Read and return the data\n  read_csv(fname, show_col_types=FALSE)\n}\n\n# Download industry codes lookup table\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\n\n\nShow Code\n# Download BLS Quarterly Census of Employment and Wages (QCEW)\n# =============================================================\n# The QCEW provides detailed employment and wage data by industry and geography.\n# We use the annual averages to understand labor market conditions across CBSAs.\n\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n  fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n  fname &lt;- file.path(\"data\", \"mp02\", fname)\n  \n  YEARS &lt;- seq(start_year, end_year)\n  YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 to match ACS (COVID year)\n  \n  if(!file.exists(fname)){\n    # Download and process data for each year\n    ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n      fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n      \n      # Download annual file if not already cached\n      if(!file.exists(fname_inner)){\n        request(\"https://www.bls.gov\") |&gt; \n          req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                       glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n          req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n          req_retry(max_tries=5) |&gt;\n          req_perform(fname_inner)\n      }\n      \n      # Verify file downloaded correctly (should be ~75MB)\n      if(file.info(fname_inner)$size &lt; 755e5){\n        warning(sQuote(fname_inner), \" appears corrupted. Please delete and retry this step.\")\n      }\n      \n      # Read and filter the data\n      read_csv(fname_inner, show_col_types=FALSE) |&gt; \n        mutate(YEAR = yy) |&gt;\n        select(area_fips,           # Geographic identifier\n               industry_code,        # NAICS industry code\n               annual_avg_emplvl,    # Average employment level\n               total_annual_wages,   # Total wages paid\n               YEAR) |&gt;\n        # Filter to CBSAs only (FIPS starting with 'C') and valid industry codes\n        filter(nchar(industry_code) &lt;= 5,           # Keep only detailed industries\n               str_starts(area_fips, \"C\")) |&gt;       # Keep only CBSAs\n        filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;  # Remove ranges\n        # Rename for clarity\n        mutate(FIPS = area_fips, \n               INDUSTRY = as.integer(industry_code), \n               EMPLOYMENT = as.integer(annual_avg_emplvl), \n               TOTAL_WAGES = total_annual_wages) |&gt;\n        select(-area_fips, -industry_code, -annual_avg_emplvl, -total_annual_wages) |&gt;\n        # Remove \"all industries\" aggregate (code 10)\n        filter(INDUSTRY != 10) |&gt; \n        # Calculate average wage per employee\n        mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n    })) |&gt; bind_rows()\n    \n    # Cache the combined results (compressed to save space)\n    write_csv(ALL_DATA, fname)\n  }\n  \n  # Read the cached data\n  ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n  \n  # Verify all years downloaded successfully\n  ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n  YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n  \n  if(length(YEARS_DIFF) &gt; 0){\n    stop(\"Download failed for the following years: \", YEARS_DIFF, \n         \". Please delete intermediate files and try again.\")\n  }\n  \n  ALL_DATA\n}\n\n# Download wage data\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n\n\n2.1 Examining Data set Structure and Identifying Join Keys\nNow that we have downloaded all necessary datasets, we need to examine their structure to identify the appropriate keys for joining them together in our analysis.\n\n\nShow Code\n# Examine Census Data Structure\ncat(\"=\", rep(\"=\", 69), \"\\n\", sep=\"\")\n\n\n======================================================================\n\n\nShow Code\ncat(\"CENSUS DATA (from tidycensus - ACS)\\n\")\n\n\nCENSUS DATA (from tidycensus - ACS)\n\n\nShow Code\ncat(\"=\", rep(\"=\", 69), \"\\n\\n\", sep=\"\")\n\n\n======================================================================\n\n\nShow Code\nglimpse(INCOME)\n\n\nRows: 7,279\nColumns: 4\n$ GEOID            &lt;dbl&gt; 10140, 10180, 10300, 10380, 10420, 10500, 10540, 1058…\n$ NAME             &lt;chr&gt; \"Aberdeen, WA Micro Area\", \"Abilene, TX Metro Area\", …\n$ household_income &lt;dbl&gt; 36345, 42931, 45640, 13470, 47482, 36218, 47669, 5767…\n$ year             &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,…\n\n\nThe American Community Survey (ACS) provides four key data sets for our analysis. The INCOME data set contains 7279 observations across 593 unique Core Based Statistical Areas (CBSAs). These metro areas are tracked over 14 years, specifically: 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2021, 2022, 2023. Note that 2020 is excluded because the Census Bureau did not conduct the 1-year ACS survey that year due to the COVID-19 pandemic.\nAll four census data sets (INCOME, RENT, POPULATION, and HOUSEHOLDS) share the same structure with three common keys: GEOID (numeric CBSA identifier), NAME (metro area name), and year (survey year). These datasets can be joined directly to each other using GEOID and year as composite keys.\n\n\nShow Code\n# Examine Building Permits Data Structure\ncat(\"=\", rep(\"=\", 69), \"\\n\", sep=\"\")\n\n\n======================================================================\n\n\nShow Code\ncat(\"BUILDING PERMITS DATA\\n\")\n\n\nBUILDING PERMITS DATA\n\n\nShow Code\ncat(\"=\", rep(\"=\", 69), \"\\n\\n\", sep=\"\")\n\n\n======================================================================\n\n\nShow Code\nglimpse(PERMITS)\n\n\nRows: 5,658\nColumns: 3\n$ CBSA                        &lt;dbl&gt; 10180, 10420, 10500, 10580, 10740, 10780, …\n$ new_housing_units_permitted &lt;dbl&gt; 214, 741, 213, 1380, 1692, 396, 1648, 125,…\n$ year                        &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, …\n\n\nThe PERMITS data set tracks new housing construction permits across 401 CBSAs over 15 years (2009-2023). Unlike the census data, this data set includes 2020 data. The PERMITS data set uses CBSA as a numeric code, matching the format of GEOID in the census data, so we can join them directly: GEOID == CBSA.\n\n\nShow Code\n# Examine Industry Classification Data\ncat(\"=\", rep(\"=\", 69), \"\\n\", sep=\"\")\n\n\n======================================================================\n\n\nShow Code\ncat(\"BLS INDUSTRY CODES (NAICS Classification)\\n\")\n\n\nBLS INDUSTRY CODES (NAICS Classification)\n\n\nShow Code\ncat(\"=\", rep(\"=\", 69), \"\\n\\n\", sep=\"\")\n\n\n======================================================================\n\n\nShow Code\nglimpse(INDUSTRY_CODES)\n\n\nRows: 798\nColumns: 8\n$ level1_title &lt;chr&gt; \"NAICS 11 Agriculture, forestry, fishing and hunting\", \"N…\n$ level2_title &lt;chr&gt; \"NAICS 111 Crop production\", \"NAICS 111 Crop production\",…\n$ level3_title &lt;chr&gt; \"NAICS 1111 Oilseed and grain farming\", \"NAICS 1111 Oilse…\n$ level4_title &lt;chr&gt; \"NAICS 11111 Soybean farming\", \"NAICS 11112 Oilseed (exce…\n$ level1_code  &lt;dbl&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1…\n$ level2_code  &lt;dbl&gt; 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 11…\n$ level3_code  &lt;dbl&gt; 1111, 1111, 1111, 1111, 1111, 1111, 1111, 1112, 1113, 111…\n$ level4_code  &lt;dbl&gt; 11111, 11112, 11113, 11114, 11115, 11116, 11119, 11121, 1…\n\n\nShow Code\nhead(INDUSTRY_CODES %&gt;% select(level1_title, level4_title, level4_code), 3)\n\n\n\n  \n\n\n\nThe INDUSTRY_CODES data set is a lookup table containing 798 detailed industry classifications following the North American Industry Classification System (NAICS). This hierarchical system has four levels, from broad sectors (level 1) to specific industries (level 4). For example, level 1 includes broad categories like “NAICS 11 Agriculture, forestry, fishing and hunting” while level 4 provides specific industries like “NAICS 11111 Soybean farming”. This table will be essential for understanding the wage data by industry type, joining on level4_code == INDUSTRY.\n\n\nShow Code\n# Examine BLS Wage Data Structure\ncat(\"=\", rep(\"=\", 69), \"\\n\", sep=\"\")\n\n\n======================================================================\n\n\nShow Code\ncat(\"BLS QCEW WAGE DATA\\n\")\n\n\nBLS QCEW WAGE DATA\n\n\nShow Code\ncat(\"=\", rep(\"=\", 69), \"\\n\\n\", sep=\"\")\n\n\n======================================================================\n\n\nShow Code\nglimpse(WAGES)\n\n\nRows: 4,442,181\nColumns: 6\n$ YEAR        &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009…\n$ FIPS        &lt;chr&gt; \"C1018\", \"C1018\", \"C1018\", \"C1018\", \"C1018\", \"C1018\", \"C10…\n$ INDUSTRY    &lt;dbl&gt; 101, 1011, 1012, 1013, 102, 1021, 1022, 1023, 1024, 1025, …\n$ EMPLOYMENT  &lt;dbl&gt; 8050, 1769, 3328, 2952, 42334, 11993, 0, 3575, 4697, 11950…\n$ TOTAL_WAGES &lt;dbl&gt; 342280951, 86660038, 151822573, 103798340, 1284997543, 368…\n$ AVG_WAGE    &lt;dbl&gt; 42519.37, 48988.15, 45619.76, 35162.04, 30353.79, 30764.23…\n\n\nShow Code\ncat(\"\\nSample FIPS codes:\", head(unique(WAGES$FIPS), 5), \"\\n\")\n\n\n\nSample FIPS codes: C1018 C1038 C1042 C1050 C1058 \n\n\nThe WAGES data set from the BLS Quarterly Census of Employment and Wages is our largest data set with 4,442,181 observations. It tracks employment and wages across 405 geographic areas, 1227 industries, and 14 years.\nThe joining strategy here is more complex because the FIPS column stores CBSA codes with a “C” prefix (e.g., “C10180”). To join with census data, we need to extract the numeric portion: as.numeric(substr(FIPS, 2, 6)). Additionally, the year column is named YEAR (uppercase) rather than year (lowercase), so we’ll need to account for this when joining.\n\n\nShow Code\n# Create summary table of all data sets\ndata_summary &lt;- tibble(\n  Dataset = c(\"INCOME/RENT/POPULATION/HOUSEHOLDS\", \"PERMITS\", \"WAGES\", \"INDUSTRY_CODES\"),\n  Observations = c(format(nrow(INCOME), big.mark=\",\"), \n                   format(nrow(PERMITS), big.mark=\",\"), \n                   format(nrow(WAGES), big.mark=\",\"), \n                   format(nrow(INDUSTRY_CODES), big.mark=\",\")),\n  Geographic_Units = c(n_distinct(INCOME$GEOID), \n                       n_distinct(PERMITS$CBSA), \n                       n_distinct(WAGES$FIPS), \n                       \"N/A (Lookup Table)\"),\n  Time_Periods = c(n_distinct(INCOME$year), \n                   n_distinct(PERMITS$year), \n                   n_distinct(WAGES$YEAR), \n                   \"N/A\"),\n  Primary_Keys = c(\"GEOID + year\", \n                   \"CBSA + year\", \n                   \"FIPS + YEAR + INDUSTRY\", \n                   \"level4_code\")\n)\n\nknitr::kable(data_summary, \n             caption = \"Summary of All Datasets\",\n             align = c('l', 'r', 'r', 'r', 'l'))\n\n\n\nSummary of All Datasets\n\n\n\n\n\n\n\n\n\nDataset\nObservations\nGeographic_Units\nTime_Periods\nPrimary_Keys\n\n\n\n\nINCOME/RENT/POPULATION/HOUSEHOLDS\n7,279\n593\n14\nGEOID + year\n\n\nPERMITS\n5,658\n401\n15\nCBSA + year\n\n\nWAGES\n4,442,181\n405\n14\nFIPS + YEAR + INDUSTRY\n\n\nINDUSTRY_CODES\n798\nN/A (Lookup Table)\nN/A\nlevel4_code\n\n\n\n\n\nSummary of Joining Strategy:\n\nCensus data sets (INCOME, RENT, POPULATION, HOUSEHOLDS) can be joined directly on GEOID and year\nPERMITS joins to census data where GEOID == CBSA and year matches\n\nWAGES requires extracting numeric CBSA from FIPS column, then joining on the converted value and matching year\nINDUSTRY_CODES joins to WAGES where INDUSTRY == level4_code to add readable industry names\n\nNow we can continue to combine these data sets for our YIMBY analysis."
  },
  {
    "objectID": "mp02.html#exploratory-analysis",
    "href": "mp02.html#exploratory-analysis",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "3 Exploratory Analysis",
    "text": "3 Exploratory Analysis\n\n3.1 Entity Relationship Diagram (ERD)\nTo better understand the relationships between these data sets, I’ve created an Entity Relationship Diagram (ERD) using the DiagrammeR package. This shows the structure of each table and their relationships.\n\n\nShow Code\n# Install and load DiagrammeR package for creating the ERD\nif(!require(DiagrammeR, quietly = TRUE)) {\n  install.packages(\"DiagrammeR\")\n}\nlibrary(DiagrammeR)\n\n\n\n\nShow Code\nlibrary(DiagrammeR)\n\ngrViz(\"\ndigraph ERD {\n  graph [rankdir=LR, fontname=Helvetica, bgcolor=white]\n  node [shape=rectangle, style=filled, fontname=Helvetica, fontsize=10, margin=0.2]\n  edge [fontname=Helvetica, fontsize=8]\n  \n  # Census datasets\n  INCOME [label='INCOME\\n━━━━━━\\nGEOID (PK)\\nNAME\\nyear (PK)\\n━━━━━━\\nhousehold_income', fillcolor='#E8F4F8']\n  RENT [label='RENT\\n━━━━━━\\nGEOID (PK)\\nNAME\\nyear (PK)\\n━━━━━━\\nmonthly_rent', fillcolor='#E8F4F8']\n  POPULATION [label='POPULATION\\n━━━━━━\\nGEOID (PK)\\nNAME\\nyear (PK)\\n━━━━━━\\npopulation', fillcolor='#E8F4F8']\n  HOUSEHOLDS [label='HOUSEHOLDS\\n━━━━━━\\nGEOID (PK)\\nNAME\\nyear (PK)\\n━━━━━━\\nhouseholds', fillcolor='#E8F4F8']\n  \n  # Other datasets\n  PERMITS [label='PERMITS\\n━━━━━━\\nCBSA (PK)\\nyear (PK)\\n━━━━━━\\nnew_housing_units', fillcolor='#FFF4E6']\n  WAGES [label='WAGES\\n━━━━━━\\nFIPS (PK)\\nYEAR (PK)\\nINDUSTRY (PK)\\n━━━━━━\\nEMPLOYMENT\\nTOTAL_WAGES\\nAVG_WAGE', fillcolor='#F0E6FF']\n  INDUSTRY_CODES [label='INDUSTRY_CODES\\n━━━━━━\\nlevel4_code (PK)\\n━━━━━━\\nlevel1-4_title\\nlevel1-4_code', fillcolor='#E6F7E6']\n  \n  # Relationships\n  INCOME -&gt; RENT [label='GEOID+year', color='#0066CC', penwidth=2]\n  INCOME -&gt; POPULATION [label='GEOID+year', color='#0066CC', penwidth=2]\n  INCOME -&gt; HOUSEHOLDS [label='GEOID+year', color='#0066CC', penwidth=2]\n  INCOME -&gt; PERMITS [label='GEOID=CBSA\\n+year', color='#FF9900', style=dashed, penwidth=2]\n  INCOME -&gt; WAGES [label='GEOID=\\nsubstr(FIPS,2)\\n+year=YEAR', color='#9933FF', style=dashed, penwidth=2]\n  WAGES -&gt; INDUSTRY_CODES [label='INDUSTRY=\\nlevel4_code', color='#009900', penwidth=2]\n}\n\")\n\n\n\n\n\n\n\n\nFigure 1: Entity Relationship Diagram created with DiagrammeR\n\n\n\n\nKey Insights from the ERD:\nThe diagram reveals three distinct relationship patterns in our data:\n\nDirect Relationships (Census Data): The four ACS data sets (INCOME, RENT, POPULATION, HOUSEHOLDS) share identical structures with 593 CBSAs tracked over 14 years. They can be joined directly using composite keys (GEOID + year), making them straightforward to combine.\nSimple Type Conversion (PERMITS): The building permits data uses CBSA as a numeric identifier, which directly matches the GEOID in census data. This allows for a direct join: GEOID == CBSA, though we must be mindful that PERMITS includes 2020 data while census data does not.\nComplex String Manipulation (WAGES): The wage data presents the most complex joining challenge. The FIPS column stores CBSA codes with a “C” prefix (e.g., “C10180” for Abilene, TX), requiring string extraction via as.numeric(substr(FIPS, 2, 6)) to match with census GEOID values. Additionally, the year column is uppercase (YEAR vs year), necessitating careful attention during joins.\nLookup Table Enhancement (INDUSTRY_CODES): The NAICS industry classification table serves as a hierarchical lookup, allowing us to enrich the wage data with human-readable industry names and broader sector classifications through the INDUSTRY == level4_code relationship.\n\nThis relationship structure will guide our data integration strategy in the following sections, where we’ll progressively build a comprehensive data set combining housing affordability metrics, construction activity, and labor market conditions across US metropolitan areas."
  },
  {
    "objectID": "mp02.html#final-insights-and-deliverable",
    "href": "mp02.html#final-insights-and-deliverable",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "Final Insights and Deliverable",
    "text": "Final Insights and Deliverable\nCode related to the final deliverable of the assignment goes here.\n\nThis work ©2025 by cguirand27 was initially prepared as a Mini-Project for STA 9750 at Baruch College. More details about this course can be found at the course site and instructions for this assignment can be found at MP #02"
  },
  {
    "objectID": "mp02.html#data-integration-and-initial-exploration",
    "href": "mp02.html#data-integration-and-initial-exploration",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "4 Data Integration and Initial Exploration",
    "text": "4 Data Integration and Initial Exploration\n\n4.1 Task 2 Questions\n\n\n4.2 Question 1: Largest Housing Construction (2010-2019)\nQuestion: Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nShow Code\n# Filter permits data to the decade 2010-2019 and sum by CBSA\n# Then joins with census data to get the readable CBSA name\n\nq1_result &lt;- PERMITS |&gt;\n  # Filter to the specified decade\n  filter(year &gt;= 2010, year &lt;= 2019) |&gt;\n  # Group by CBSA and sum all permits over the decade\n  group_by(CBSA) |&gt;\n  summarize(total_permits = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt;\n  # Sort to find the top CBSA\n  arrange(desc(total_permits)) |&gt;\n  # Take the top result\n  slice(1) |&gt;\n  # Join with INCOME (or any census table) to get the CBSA name\n  # Use distinct to get just one row per CBSA (since census has multiple years)\n  left_join(\n    INCOME |&gt; select(GEOID, NAME) |&gt; distinct(),\n    by = c(\"CBSA\" = \"GEOID\")\n  )\n\n# Display the result\nq1_result\n\n\n\n  \n\n\n\nAnswer: The CBSA that permitted the most new housing units between 2010 and 2019 was Houston-Sugar Land-Baytown, TX Metro Area, Houston-The Woodlands-Sugar Land, TX Metro Area, Houston-Pasadena-The Woodlands, TX Metro Area, with a total of 482,075, 482,075, 482,075 units permitted during this decade. This reflects the major population growth and housing demand this metropolitan area saw during the 2010s.\n\n\n\n4.3 Question 2: Albuquerque’s Peak Construction Year\nQuestion: In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nShow Code\n# Filter to Albuquerque CBSA and find the year with maximum permits\n\nq2_result &lt;- PERMITS |&gt;\n  # Filter to Albuquerque only\n  filter(CBSA == 10740) |&gt;\n  # Sort by permits to find the maximum\n  arrange(desc(new_housing_units_permitted)) |&gt;\n  # Take the top year\n  slice(1)\n\n# Display the result\nq2_result\n\n\n\n  \n\n\n\nShow Code\n# Also create a visualization to see the trend and identify the artifact\nalbuquerque_trend &lt;- PERMITS |&gt;\n  filter(CBSA == 10740) |&gt;\n  arrange(year)\n\n# Show the full time series to identify any anomalies\nalbuquerque_trend\n\n\n\n  \n\n\n\nAnswer: According to the data, Albuquerque permitted the most housing units in 2021, with 4,021 units.\nImportant Note on COVID-19 Data Artifact: Based on this time series, we should be cautious about this result since the year 2021 coincides with the COVID-19 pandemic period. This might’ve affected data collection and reporting. If 2020 shows an unusually high value, this may represent a data quality issue rather than actual construction activity. A more reliable answer would exclude 2020 and focus on years with normal data collection: the peak non-pandemic year was 2021 with 4,021 units.\n\n\n\n4.4 Question 3: Highest Average Income by State (2015)\nQuestion: Which state (not CBSA) had the highest average individual income in 2015?\n\n\nShow Code\n# Create state lookup table for converting abbreviations to full names\nstate_df &lt;- data.frame(\n  abb  = c(state.abb, \"DC\", \"PR\"),\n  name = c(state.name, \"District of Columbia\", \"Puerto Rico\")\n)\n\n# Calculate total income and population by state for 2015\nq3_result &lt;- INCOME |&gt;\n  # Filter to 2015 only\n  filter(year == 2015) |&gt;\n  # Join with HOUSEHOLDS to get household counts\n  inner_join(\n    HOUSEHOLDS |&gt; filter(year == 2015),\n    by = c(\"GEOID\", \"NAME\", \"year\")\n  ) |&gt;\n  # Join with POPULATION to get total population\n  inner_join(\n    POPULATION |&gt; filter(year == 2015),\n    by = c(\"GEOID\", \"NAME\", \"year\")\n  ) |&gt;\n  # Extract the principal state from CBSA name\n  mutate(state = str_extract(NAME, \", (.{2})\", group = 1)) |&gt;\n  # Calculate total income per CBSA (household_income * households)\n  mutate(total_income = household_income * households) |&gt;\n  # Group by state and sum income and population\n  group_by(state) |&gt;\n  summarize(\n    total_state_income = sum(total_income, na.rm = TRUE),\n    total_state_population = sum(population, na.rm = TRUE)\n  ) |&gt;\n  # Calculate average individual income per state\n  mutate(avg_individual_income = total_state_income / total_state_population) |&gt;\n  # Join with state names for readability\n  left_join(state_df, by = c(\"state\" = \"abb\")) |&gt;\n  # Sort by average income\n  arrange(desc(avg_individual_income)) |&gt;\n  # Take the top state\n  slice(1)\n\n# Display the result\nq3_result\n\n\n\n  \n\n\n\nAnswer: In 2015, District of Columbia (state abbreviation: DC) had the highest average individual income at $33,232.88. This was calculated by summing total household income across all CBSAs in the state (income × households) and dividing by the total population, giving us the average income per person rather than per household.\n\n\n\n4.5 Question 4: Data Scientists Employment - NYC vs San Francisco\nQuestion: What is the last year in which the NYC CBSA had the most data scientists in the country?\nFirst, we need to create a compatible CBSA code in the WAGES data to join with census data:\n\n\nShow Code\n# Transform BLS FIPS codes to match Census GEOID format\n# BLS uses \"C1234\" while Census uses \"12340\" (numeric, with trailing 0)\nWAGES_clean &lt;- WAGES |&gt;\n  mutate(\n    # Remove the \"C\" prefix, convert to numeric, then multiply by 10 to add trailing 0\n    CBSA_code = as.numeric(str_remove(FIPS, \"C\")) * 10\n  )\n\n# Verify the transformation worked\ncat(\"Sample FIPS codes from WAGES:\", head(WAGES$FIPS, 3), \"\\n\")\n\n\nSample FIPS codes from WAGES: C1018 C1018 C1018 \n\n\nShow Code\ncat(\"Converted to CBSA codes:\", head(WAGES_clean$CBSA_code, 3), \"\\n\")\n\n\nConverted to CBSA codes: 10180 10180 10180 \n\n\nShow Code\ncat(\"Sample GEOID from census:\", head(INCOME$GEOID, 3), \"\\n\")\n\n\nSample GEOID from census: 10140 10180 10300 \n\n\nNow we find when NYC had the most data scientists:\n\n\nShow Code\n# NAICS code 5182 = Data Processing, Hosting, and Related Services\n# (This is the code mentioned for data scientists and business analysts)\n\n# Find which CBSA had the most data scientists each year\nq4_result &lt;- WAGES_clean |&gt;\n  # Filter to data scientists industry only\n  filter(INDUSTRY == 5182) |&gt;\n  # For each year, find the CBSA with maximum employment\n  group_by(YEAR) |&gt;\n  arrange(desc(EMPLOYMENT)) |&gt;\n  slice(1) |&gt;\n  ungroup() |&gt;\n  # Join with census data to get CBSA names\n  left_join(\n    INCOME |&gt; select(GEOID, NAME) |&gt; distinct(),\n    by = c(\"CBSA_code\" = \"GEOID\")\n  ) |&gt;\n  # Select relevant columns for display\n  select(YEAR, NAME, CBSA_code, EMPLOYMENT) |&gt;\n  arrange(YEAR)\n\n\nWarning in left_join(ungroup(slice(arrange(group_by(filter(WAGES_clean, : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 112 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nShow Code\n# Display the full table\nq4_result\n\n\n\n  \n\n\n\nShow Code\n# Find the last year NYC was on top\nnyc_last_year &lt;- q4_result |&gt;\n  filter(str_detect(NAME, \"New York\")) |&gt;\n  arrange(desc(YEAR)) |&gt;\n  slice(1)\n\n\nAnswer: Based on the analysis, the last year when the NYC CBSA had the most data scientists was 2015, with 18,922 employees in NAICS industry 5182. After this year, San Francisco (or another CBSA) took the lead in data science employment, reflecting the tech industry’s concentration on the West Coast.\nThe table above shows the CBSA with the most data scientists for each year in our data set, illustrating the shift in where data science jobs are concentrated.\n\n\n\n4.6 Question 5: Finance Industry Wages in NYC\nQuestion: What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nShow Code\n# First, identify NYC's CBSA code\nnyc_info &lt;- INCOME |&gt;\n  filter(str_detect(NAME, \"New York-Newark-Jersey City\")) |&gt;\n  select(GEOID, NAME) |&gt;\n  distinct()\n\nnyc_cbsa &lt;- nyc_info$GEOID[1]\ncat(\"NYC CBSA code:\", nyc_cbsa, \"\\n\")\n\n\nNYC CBSA code: 35620 \n\n\nShow Code\n# Calculate finance wages as fraction of total wages each year\nq5_result &lt;- WAGES_clean |&gt;\n  # Filter to NYC only\n  filter(CBSA_code == nyc_cbsa) |&gt;\n  # Create indicator for finance industry (NAICS 52 = level 2 code)\n  # Industry codes starting with 52 are finance\n  mutate(is_finance = str_starts(as.character(INDUSTRY), \"52\")) |&gt;\n  # Group by year and calculate totals\n  group_by(YEAR) |&gt;\n  summarize(\n    finance_wages = sum(TOTAL_WAGES[is_finance], na.rm = TRUE),\n    total_wages = sum(TOTAL_WAGES, na.rm = TRUE),\n    finance_employment = sum(EMPLOYMENT[is_finance], na.rm = TRUE),\n    total_employment = sum(EMPLOYMENT, na.rm = TRUE)\n  ) |&gt;\n  # Calculate the fraction\n  mutate(\n    finance_wage_fraction = finance_wages / total_wages,\n    finance_emp_fraction = finance_employment / total_employment\n  ) |&gt;\n  arrange(YEAR)\n\n# Display the results\nq5_result\n\n\n\n  \n\n\n\nShow Code\n# Find the peak year\npeak_year &lt;- q5_result |&gt;\n  arrange(desc(finance_wage_fraction)) |&gt;\n  slice(1)\n\n\nAnswer: In the NYC CBSA, the finance and insurance industry’s share of total wages peaked in 2021, when it accounted for 15.87% of all wages paid in the metro area. At that time, the finance sector employed 5.11% of the workforce but earned a disproportionately large share of total wages, which is reflected in the high salaries in this industry.\nOver the entire time period, the finance industry’s wage share in NYC ranged from 9.4% to 15.87%, demonstrating the continued importance of the financial sector to New York’s economy, even as its relative dominance has evolved over time.\n\n\n4.7 Task 3: Initial Visualizations\nThe following visualizations will help us explore housing costs, employment patterns, and household characteristics across US metropolitan areas, and therefore, better understand the relationships in our data.\n\n\n4.8 Visualization 1: Rent vs. Income Relationship (2009)\nQuestion: What is the relationship between monthly rent and average household income per CBSA in 2009?\n\n\nShow Code\n# Join RENT and INCOME data for 2009\nrent_income_2009 &lt;- RENT |&gt;\n  filter(year == 2009) |&gt;\n  inner_join(\n    INCOME |&gt; filter(year == 2009),\n    by = c(\"GEOID\", \"NAME\", \"year\")\n  ) |&gt;\n  # Remove any missing values\n  filter(!is.na(monthly_rent), !is.na(household_income))\n\n# Create the visualization\nggplot(rent_income_2009, aes(x = household_income, y = monthly_rent)) +\n  # Add points with some transparency to handle over plotting\n  geom_point(alpha = 0.6, color = \"#2C3E50\", size = 2) +\n  # Add a smooth trend line to show the relationship\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#E74C3C\", linewidth = 1.2) +\n  # Format axis labels with dollar signs and comma separators\n  scale_x_continuous(\n    labels = scales::dollar_format(),\n    breaks = seq(0, 100000, 20000)\n  ) +\n  scale_y_continuous(\n    labels = scales::dollar_format(),\n    breaks = seq(0, 2000, 250)\n  ) +\n  # Add proper labels and title\n  labs(\n    title = \"Relationship Between Household Income and Monthly Rent (2009)\",\n    subtitle = \"Each point represents a Core Based Statistical Area (CBSA)\",\n    x = \"Median Household Income (Annual)\",\n    y = \"Median Monthly Gross Rent\",\n    caption = \"Data Source: American Community Survey (ACS) 2009\"\n  ) +\n  # Use a clean theme with larger text\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nShow Code\n# Calculate correlation for narrative\ncorrelation &lt;- cor(rent_income_2009$household_income, \n                   rent_income_2009$monthly_rent, \n                   use = \"complete.obs\")\n\n\nObservation: There is a strong positive relationship (correlation = 0.764) between household income and monthly rent across CBSAs in 2009. Metropolitan areas with higher median household incomes tend to have higher median rents. Areas with higher-paying jobs attract more residents, driving up housing costs. The relationship seems to be somewhat linear, despite a considerable amount of variation around the trend line. This suggests that local factors beyond income (such as housing supply, geography, and regulations) also play important roles in determining rent levels.\n\n\n\n4.9 Visualization 2: Healthcare Employment Over Time\nQuestion: What is the relationship between total employment and healthcare/social services employment across CBSAs over time?\n\n\nShow Code\n# Prepare wage data with CBSA codes\nWAGES_clean &lt;- WAGES |&gt;\n  mutate(CBSA_code = as.numeric(str_remove(FIPS, \"C\")) * 10)\n\n# Calculate total employment and healthcare employment by CBSA and year\nhealthcare_employment &lt;- WAGES_clean |&gt;\n  # Healthcare and Social Services is NAICS 62 (all codes starting with 62)\n  mutate(is_healthcare = str_starts(as.character(INDUSTRY), \"62\")) |&gt;\n  group_by(CBSA_code, YEAR) |&gt;\n  summarize(\n    total_employment = sum(EMPLOYMENT, na.rm = TRUE),\n    healthcare_employment = sum(EMPLOYMENT[is_healthcare], na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  # Calculate healthcare as percentage of total\n  mutate(healthcare_pct = healthcare_employment / total_employment * 100) |&gt;\n  # Filter out CBSAs with very small employment (potential data issues)\n  filter(total_employment &gt;= 10000)\n\n# Create the visualization showing evolution over time\nggplot(healthcare_employment, \n       aes(x = total_employment, y = healthcare_employment, color = as.factor(YEAR))) +\n  # Use points with transparency\n  geom_point(alpha = 0.5, size = 1.5) +\n  # Add a reference line showing constant healthcare share\n  geom_abline(slope = 0.15, intercept = 0, linetype = \"dashed\", \n              color = \"gray40\", linewidth = 0.8) +\n  # Format axes with comma separators\n  scale_x_continuous(\n    labels = scales::comma_format(),\n    trans = \"log10\"  # Log scale helps show the relationship across different sized CBSAs\n  ) +\n  scale_y_continuous(\n    labels = scales::comma_format(),\n    trans = \"log10\"\n  ) +\n  # Use a color palette that shows time progression\n  scale_color_viridis_d(option = \"plasma\", name = \"Year\") +\n  # Add proper labels\n  labs(\n    title = \"Healthcare Employment vs. Total Employment Across US Metro Areas\",\n    subtitle = \"Healthcare sector consistently represents ~15% of total employment (dashed line)\",\n    x = \"Total Employment (log scale)\",\n    y = \"Healthcare & Social Services Employment (log scale)\",\n    caption = \"Data Source: BLS Quarterly Census of Employment and Wages (QCEW)\\nNote: CBSAs with &lt;10,000 total employment excluded\"\n  ) +\n  # Clean theme\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"right\",\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nShow Code\n# Calculate average healthcare share across all years\navg_healthcare_share &lt;- mean(healthcare_employment$healthcare_pct, na.rm = TRUE)\n\n\nObservation: Healthcare and social services employment shows a consistent relationship with total employment throughout metro areas and over time. On average, the healthcare sector accounts for approximately 10.1% of total employment across all CBSAs in our data set. The dashed reference line (representing 15% of total employment) shows that this relationship is relatively stable throughout metro areas of different sizes. The coloring by year reveals that this relationship has remained stable over time, though there may be a slight upward trend in healthcare’s share of employment in recent years, reflecting the aging population and expansion of healthcare services.\n\n\n\n4.10 Visualization 3: Evolution of Household Size Over Time\nQuestion: How has average household size evolved over time across different CBSAs?\n\n\nShow Code\n# Calculate average household size (population / households) by CBSA and year\nhousehold_size &lt;- POPULATION |&gt;\n  inner_join(\n    HOUSEHOLDS,\n    by = c(\"GEOID\", \"NAME\", \"year\")\n  ) |&gt;\n  mutate(avg_household_size = population / households) |&gt;\n  # Filter out potential data errors (unrealistic household sizes)\n  filter(avg_household_size &gt; 1, avg_household_size &lt; 5)\n\n# Install and load gghighlight for improved visualization\nlibrary(gghighlight)\n\n# Identify NYC and LA for highlighting\nhousehold_size_highlight &lt;- household_size |&gt;\n  mutate(\n    # Create indicator for highlighting\n    highlight_metro = case_when(\n      str_detect(NAME, \"New York-Newark-Jersey City\") ~ \"New York\",\n      str_detect(NAME, \"Los Angeles-Long Beach-Anaheim\") ~ \"Los Angeles\",\n      TRUE ~ NA_character_\n    )\n  )\n\n# Create the visualization with gghighlight\nggplot(household_size_highlight, \n       aes(x = year, y = avg_household_size, group = GEOID, color = highlight_metro)) +\n  geom_line(linewidth = 0.8, alpha = 0.8) +\n  # Use gghighlight to automatically dim non-highlighted lines\n  gghighlight(\n    !is.na(highlight_metro),  # Highlight only NYC and LA\n    use_direct_label = TRUE,   # Add labels directly on the lines\n    label_params = list(size = 4, fontface = \"bold\", nudge_y = 0.05),\n    unhighlighted_params = list(color = \"gray80\", alpha = 0.3, linewidth = 0.3)\n  ) +\n  # Manual colors for the highlighted metros\n  scale_color_manual(\n    values = c(\"New York\" = \"#E74C3C\", \"Los Angeles\" = \"#3498DB\"),\n    na.value = \"gray80\",\n    name = \"Highlighted Metro\"\n  ) +\n  # Format y-axis\n  scale_y_continuous(\n    breaks = seq(2.0, 3.5, 0.25),\n    limits = c(2.0, 3.5)\n  ) +\n  # Format x-axis\n  scale_x_continuous(breaks = seq(2009, 2023, 2)) +\n  # Add proper labels\n  labs(\n    title = \"Evolution of Average Household Size Across US Metro Areas (2009-2023)\",\n    subtitle = \"New York and Los Angeles highlighted; gray lines show all other CBSAs for context\",\n    x = \"Year\",\n    y = \"Average Household Size (Persons per Household)\",\n    caption = \"Data Source: American Community Survey (ACS)\\nNote: 2020 data unavailable due to COVID-19 survey suspension\"\n  ) +\n  # Clean theme\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"none\",  # gghighlight adds direct labels\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: GEOID\n\n\nWarning: Removed 63 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nShow Code\n# Calculate overall trend\noverall_trend &lt;- household_size |&gt;\n  group_by(year) |&gt;\n  summarize(national_avg = mean(avg_household_size, na.rm = TRUE))\n\nstart_size &lt;- overall_trend$national_avg[1]\nend_size &lt;- overall_trend$national_avg[nrow(overall_trend)]\nchange_pct &lt;- ((end_size - start_size) / start_size) * 100\n\n# Get specific values for NYC and LA\nnyc_data &lt;- household_size_highlight |&gt;\n  filter(highlight_metro == \"New York\") |&gt;\n  filter(year %in% c(2009, 2023))\n\nla_data &lt;- household_size_highlight |&gt;\n  filter(highlight_metro == \"Los Angeles\") |&gt;\n  filter(year %in% c(2009, 2023))\n\n\nInterpretation: Average household size has remained relatively stable across most US metro areas from 2009 to 2023, hovering around 2.5 to 2.7 persons per household. The national average declined slightly from 2.65 persons per household in 2009 to 2.52 in 2023, a change of -5%.\nBy highlighting New York and Los Angeles, we can see these major metros in context: - New York has maintained a household size around 2.75 persons, slightly below the national average, reflecting its high proportion of single-person households and smaller living spaces. - Los Angeles shows a household size around 3.01 persons, slightly above the national average, typical of West Coast metros with more family-oriented housing stock.\nThe gray background lines provide important context: they show that while NYC and LA follow the national trend, most metros are experiencing similar declines in household size.≥ This trend has important implications for housing policy: even as population grows, the number of households grows faster, increasing housing demand beyond what raw population growth would suggest.\nThe visualization reveals interesting patterns: - Most CBSAs (shown in gray) follow similar trajectories, suggesting nationwide demographic trends. - The gap in 2020 (due to COVID-19 survey suspension) is clear. - Major metro areas (colored lines) show some variation, with some southwestern metros maintaining slightly larger household sizes. - The overall downward trend reflects continued demographic shifts including delayed marriage, lower birth rates, and increasing numbers of single-person households.\nThis trend has important implications for housing policy: even as population grows, the number of households grows faster, increasing housing demand beyond what raw population growth would suggest."
  },
  {
    "objectID": "mp02.html#task-4-rent-burden-index",
    "href": "mp02.html#task-4-rent-burden-index",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "5 Task 4: Rent Burden Index",
    "text": "5 Task 4: Rent Burden Index\nIn order to understand housing affordability, we need to measure the relationship between what people earn and what they pay for housing. A common measure is the rent-to-income ratio, which expresses monthly rent as a percentage of monthly household income.\n\nConstructing the Base Metric\n\n\nShow Code\n# Join INCOME and RENT tables to calculate rent burden\nrent_burden_raw &lt;- INCOME |&gt;\n  inner_join(RENT, by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  # Calculate monthly household income (annual income / 12)\n  mutate(monthly_income = household_income / 12) |&gt;\n  # Calculate rent-to-income ratio (as a percentage)\n  mutate(rent_to_income_pct = (monthly_rent / monthly_income) * 100) |&gt;\n  # Filter out any extreme outliers or missing values\n  filter(!is.na(rent_to_income_pct), \n         rent_to_income_pct &gt; 0, \n         rent_to_income_pct &lt; 100)  # Over 100% is likely a data error\n\n# Examine the distribution\nsummary(rent_burden_raw$rent_to_income_pct)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.31   17.38   19.21   19.66   21.56   38.19 \n\n\nThe rent-to-income ratio shows that metro areas typically see median households spending between 12.3% and 38.2% of their income on rent, with an average of 19.7%.\n\n\nStandardization and Scaling\nTo create a more understandable index, we’ll standardize our metric using the national average in 2009 (our baseline year) as the reference point. We’ll set this baseline to 100, making our index easy to interpret:\n\nIndex = 100: Rent burden equal to 2009 national average\nIndex &gt; 100: Higher rent burden than 2009 baseline (less affordable)\nIndex &lt; 100: Lower rent burden than 2009 baseline (more affordable)\n\n\n\nShow Code\n# Calculate the 2009 national baseline (weighted by population for accuracy)\nbaseline_2009 &lt;- rent_burden_raw |&gt;\n  filter(year == 2009) |&gt;\n  # Join with population to weight by metro area size\n  inner_join(POPULATION |&gt; filter(year == 2009), by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  summarize(\n    # Weight each CBSA's rent burden by its population\n    baseline = weighted.mean(rent_to_income_pct, w = population, na.rm = TRUE)\n  ) |&gt;\n  pull(baseline)\n\ncat(\"2009 National Baseline Rent-to-Income Ratio:\", round(baseline_2009, 2), \"%\\n\")\n\n\n2009 National Baseline Rent-to-Income Ratio: 20.05 %\n\n\nShow Code\n# Create the standardized Rent Burden Index\nrent_burden_index &lt;- rent_burden_raw |&gt;\n  mutate(\n    # Standardize to baseline = 100\n    rent_burden_index = (rent_to_income_pct / baseline_2009) * 100\n  ) |&gt;\n  # Add interpretive categories\n  mutate(\n    affordability_category = case_when(\n      rent_burden_index &lt; 80 ~ \"Very Affordable\",\n      rent_burden_index &lt; 95 ~ \"Affordable\",\n      rent_burden_index &lt; 105 ~ \"Average\",\n      rent_burden_index &lt; 120 ~ \"Expensive\",\n      TRUE ~ \"Very Expensive\"\n    ),\n    affordability_category = factor(\n      affordability_category,\n      levels = c(\"Very Affordable\", \"Affordable\", \"Average\", \"Expensive\", \"Very Expensive\")\n    )\n  )\n\n# Summary statistics\ncat(\"\\nRent Burden Index Statistics:\\n\")\n\n\n\nRent Burden Index Statistics:\n\n\nShow Code\nsummary(rent_burden_index$rent_burden_index)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  61.41   86.66   95.80   98.07  107.51  190.49 \n\n\nOur Rent Burden Index is now centered at 100 (the 2009 national average). Values above 100 indicate that renters in that metro area face higher rent burdens relative to income than the 2009 national average, while values below 100 indicate lower burdens.\n\n\nTemporal Evolution: New York Metro Area\nLet’s examine how rent burden has evolved in one of America’s largest and most expensive housing markets: New York City.\n\n\nShow Code\n# Filter to NYC metro area\nnyc_rent_burden &lt;- rent_burden_index |&gt;\n  filter(str_detect(NAME, \"New York-Newark-Jersey City\")) |&gt;\n  arrange(year) |&gt;\n  select(year, NAME, household_income, monthly_rent, \n         rent_to_income_pct, rent_burden_index, affordability_category)\n\n# Create interactive table with DT\nlibrary(DT)\n\ndatatable(\n  nyc_rent_burden,\n  rownames = FALSE,\n  options = list(\n    pageLength = 15,\n    dom = 't',  # Just the table, no search/pagination needed for small dataset\n    ordering = TRUE\n  ),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold; font-size: 14px;',\n    'Rent Burden Evolution: New York-Newark-Jersey City Metro Area (2009-2023)'\n  )\n) |&gt;\n  formatCurrency(c(\"household_income\", \"monthly_rent\"), digits = 0) |&gt;\n  formatRound(c(\"rent_to_income_pct\", \"rent_burden_index\"), digits = 1) |&gt;\n  formatStyle(\n    'rent_burden_index',\n    backgroundColor = styleInterval(\n      cuts = c(95, 105, 120),\n      values = c('#d4edda', '#fff3cd', '#f8d7da', '#f5c6cb')\n    )\n  )\n\n\n\n\n\n\nKey Findings for NYC: The New York metro area consistently ranks as one of the most rent-burdened markets in the country. The rent burden index for NYC has decreased from 112.5 in 2009 to 110.9 in 2023, indicating that housing affordability has improved relative to the national baseline. Throughout this period, NYC residents have consistently spent 22.3% of their household income on rent on average.\n\n\nMost and Least Affordable Metro Areas (2023)\nNow let’s identify which metro areas currently have the highest and lowest rent burdens:\n\n\nShow Code\n# Get most recent year's data\nmost_recent_year &lt;- max(rent_burden_index$year)\n\n# Top 15 most expensive (highest rent burden)\nmost_expensive &lt;- rent_burden_index |&gt;\n  filter(year == most_recent_year) |&gt;\n  arrange(desc(rent_burden_index)) |&gt;\n  select(NAME, household_income, monthly_rent, rent_to_income_pct, \n         rent_burden_index, affordability_category) |&gt;\n  head(15)\n\n# Top 15 most affordable (lowest rent burden)\nmost_affordable &lt;- rent_burden_index |&gt;\n  filter(year == most_recent_year) |&gt;\n  arrange(rent_burden_index) |&gt;\n  select(NAME, household_income, monthly_rent, rent_to_income_pct, \n         rent_burden_index, affordability_category) |&gt;\n  head(15)\n\n# Display most expensive markets\ndatatable(\n  most_expensive,\n  rownames = FALSE,\n  options = list(\n    pageLength = 15,\n    dom = 't',\n    ordering = TRUE\n  ),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold; font-size: 14px;',\n    paste0('Most Rent-Burdened Metro Areas (', most_recent_year, ')')\n  )\n) |&gt;\n  formatCurrency(c(\"household_income\", \"monthly_rent\"), digits = 0) |&gt;\n  formatRound(c(\"rent_to_income_pct\", \"rent_burden_index\"), digits = 1) |&gt;\n  formatStyle(\n    'rent_burden_index',\n    backgroundColor = styleInterval(\n      cuts = c(120, 140, 160),\n      values = c('#fff3cd', '#f8d7da', '#f5c6cb', '#721c24')\n    ),\n    color = styleInterval(160, c('black', 'white'))\n  )\n\n\n\n\n\n\nAnalysis of Most Expensive Markets: The most rent-burdened metro areas in 2023 are characterized by rent burden indices well above 100, with the top market reaching 155.5. These are typically: - California coastal metros (Los Angeles, San Francisco, San Diego) - College towns with limited housing supply - Tourist destinations with vacation rental pressure - Areas with restrictive zoning and NIMBY policies\n\n\nShow Code\n# Display most affordable markets\ndatatable(\n  most_affordable,\n  rownames = FALSE,\n  options = list(\n    pageLength = 15,\n    dom = 't',\n    ordering = TRUE\n  ),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold; font-size: 14px;',\n    paste0('Most Affordable Metro Areas (', most_recent_year, ')')\n  )\n) |&gt;\n  formatCurrency(c(\"household_income\", \"monthly_rent\"), digits = 0) |&gt;\n  formatRound(c(\"rent_to_income_pct\", \"rent_burden_index\"), digits = 1) |&gt;\n  formatStyle(\n    'rent_burden_index',\n    backgroundColor = styleInterval(\n      cuts = c(60, 70, 80),\n      values = c('#155724', '#28a745', '#d4edda', '#fff3cd')\n    ),\n    color = styleInterval(70, c('white', 'black'))\n  )\n\n\n\n\n\n\nAnalysis of Most Affordable Markets: The most affordable metro areas have rent burden indices significantly below 100, with the lowest at 63.5. These tend to be: - Smaller metros in the Midwest and South - Areas with abundant land and permissive zoning - Markets with lower demand pressure - Regions with strong local economies relative to housing costs\n\n\nVisualization: Geographic Patterns in Rent Burden\n\n\nShow Code\n# Create a comparison plot showing trends in most vs least affordable\ncomparison_metros &lt;- rent_burden_index |&gt;\n  filter(\n    NAME %in% c(most_expensive$NAME[1:5], most_affordable$NAME[1:5])\n  ) |&gt;\n  mutate(\n    category = if_else(NAME %in% most_expensive$NAME[1:5], \n                       \"Most Expensive (2023)\", \n                       \"Most Affordable (2023)\"),\n    short_name = str_extract(NAME, \"^[^,]+\")  # Extract city name before comma\n  )\n\nggplot(comparison_metros, aes(x = year, y = rent_burden_index, \n                               color = short_name, linetype = category)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 100, linetype = \"dashed\", color = \"gray40\", linewidth = 0.8) +\n  annotate(\"text\", x = 2011, y = 102, label = \"2009 National Average\", \n           color = \"gray40\", size = 3) +\n  scale_color_brewer(palette = \"Paired\", name = \"Metro Area\") +\n  scale_linetype_manual(values = c(\"solid\", \"dashed\"), name = \"Category\") +\n  scale_y_continuous(breaks = seq(0, 200, 25)) +\n  labs(\n    title = \"Rent Burden Index Trends: Most vs. Least Affordable Metro Areas\",\n    subtitle = \"Index = 100 represents 2009 national average rent-to-income ratio\",\n    x = \"Year\",\n    y = \"Rent Burden Index\",\n    caption = \"Data Source: American Community Survey (ACS)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\", size = 10),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nKey Insight: The gap between the most and least affordable markets has widened over time. The most expensive markets have seen their rent burdens increase according to their baseline, while many affordable markets have maintained relatively stable (or even improving) affordability. This discrepancy suggests that housing policy challenges are not uniform across the country: some metros desperately need supply-side reforms (YIMBY policies), while others have maintained affordability through adequate housing construction."
  },
  {
    "objectID": "mp02.html#task-5-housing-growth-index",
    "href": "mp02.html#task-5-housing-growth-index",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "6 Task 5: Housing Growth Index",
    "text": "6 Task 5: Housing Growth Index\nTo identify YIMBY-friendly metros, we need to measure not just current housing construction, but whether new housing supply is keeping up with (or exceeding) population growth. A truly pro-housing metro area builds enough units to accommodate growth while also reducing the existing housing deficit.\n\nData Preparation: Calculating 5-Year Population Growth\nFirst, we’ll calculate population growth over rolling 5-year windows for each CBSA:\n\n\nShow Code\n# Join POPULATION and PERMITS data\n# Note: PERMITS uses CBSA, POPULATION uses GEOID (both numeric, directly comparable)\nhousing_growth_base &lt;- POPULATION |&gt;\n  inner_join(\n    PERMITS,\n    by = c(\"GEOID\" = \"CBSA\", \"year\" = \"year\")\n  ) |&gt;\n  arrange(GEOID, year)\n\n# Calculate 5-year population growth using lag\nhousing_growth &lt;- housing_growth_base |&gt;\n  group_by(GEOID, NAME) |&gt;\n  arrange(year) |&gt;\n  mutate(\n    # Population 5 years ago\n    population_5yr_ago = lag(population, n = 5),\n    # Absolute population change over 5 years\n    pop_change_5yr = population - population_5yr_ago,\n    # Percentage population growth over 5 years\n    pop_growth_rate_5yr = (pop_change_5yr / population_5yr_ago) * 100,\n    # Average annual population growth\n    avg_annual_pop_growth = pop_change_5yr / 5\n  ) |&gt;\n  ungroup() |&gt;\n  # Filter to years where we have 5-year lookback (2014 onwards)\n  filter(year &gt;= 2014, !is.na(pop_change_5yr))\n\n# Summary statistics\ncat(\"Summary of 5-Year Population Growth Rates:\\n\")\n\n\nSummary of 5-Year Population Growth Rates:\n\n\nShow Code\nsummary(housing_growth$pop_growth_rate_5yr)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-29.5001  -0.1085   2.9857   4.0179   6.5207  84.2809 \n\n\nStarting in 2014, we can now calculate 5-year population growth for each metro area. The average CBSA grew by 4.02% over 5-year periods, though with substantial variation across metros.\n\n\nMetric 1: Instantaneous Housing Supply (Per Capita Construction)\nOur first metric measures the rate of new construction relative to current population. This captures how actively a metro is building housing regardless of whether population is growing:\n\n\nShow Code\n# Calculate permits per 1,000 residents\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    # New housing units per 1,000 residents\n    permits_per_1000 = (new_housing_units_permitted / population) * 1000\n  )\n\n# Calculate baseline: median across all CBSA-years\nbaseline_permits_per_1000 &lt;- median(housing_growth$permits_per_1000, na.rm = TRUE)\n\ncat(\"Baseline permits per 1,000 residents:\", round(baseline_permits_per_1000, 2), \"\\n\")\n\n\nBaseline permits per 1,000 residents: 3.24 \n\n\nShow Code\n# Create standardized index (baseline = 100)\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    construction_intensity_index = (permits_per_1000 / baseline_permits_per_1000) * 100,\n    # Categorize\n    construction_category = case_when(\n      construction_intensity_index &lt; 50 ~ \"Very Low Construction\",\n      construction_intensity_index &lt; 80 ~ \"Low Construction\",\n      construction_intensity_index &lt; 120 ~ \"Moderate Construction\",\n      construction_intensity_index &lt; 200 ~ \"High Construction\",\n      TRUE ~ \"Very High Construction\"\n    )\n  )\n\ncat(\"\\nConstruction Intensity Index Statistics:\\n\")\n\n\n\nConstruction Intensity Index Statistics:\n\n\nShow Code\nsummary(housing_growth$construction_intensity_index)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.8546   57.2166  100.0000  130.0599  173.4977 1050.7545 \n\n\nThe Construction Intensity Index measures how many housing units are permitted based on population, with 100 representing the median rate. A metro with an index of 200 is permitting housing at twice the typical rate per capita.\n\n\nMetric 2: Supply-Growth Balance (Construction vs Population Growth)\nOur second metric compares new housing supply to population growth. This identifies metros that are building enough to not just accommodate growth, but potentially reduce housing scarcity:\n\n\nShow Code\n# Calculate housing permits based on population change\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    # Ratio of new housing units to population growth\n    # Values &gt; 1 mean building faster than population growth\n    # Interpretation: housing units per new resident over 5 years\n    supply_growth_ratio = (new_housing_units_permitted * 5) / pop_change_5yr\n  ) |&gt;\n  # Handle special cases\n  mutate(\n    supply_growth_ratio = case_when(\n      # Population declining but still building: very pro-housing\n      pop_change_5yr &lt;= 0 & new_housing_units_permitted &gt; 0 ~ \n        pmax(supply_growth_ratio, 5),  # Cap at 5x for visualization\n      # Population growing but no permits: anti-housing\n      pop_change_5yr &gt; 0 & new_housing_units_permitted == 0 ~ 0,\n      # Normal case\n      TRUE ~ supply_growth_ratio\n    ),\n    # Cap extreme outliers for understanding\n    supply_growth_ratio_capped = pmin(supply_growth_ratio, 5)\n  )\n\n# Calculate baseline: value of 1.0 means permits = population growth\n# We're using log transformation since ratio of 2 is as meaningful as ratio of 0.5\nbaseline_supply_growth &lt;- 1.0  # Perfect balance\n\n# Create standardized index (1.0 = 100, log-scaled for symmetry)\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    # Log transformation makes ratios symmetric (2x and 0.5x equally distant from 1)\n    supply_balance_index = (log(supply_growth_ratio_capped) / log(baseline_supply_growth)) * 100 + 100,\n    # Simpler interpretation: &gt;100 means building faster than growth\n    supply_balance_category = case_when(\n      supply_balance_index &lt; 80 ~ \"Severe Undersupply\",\n      supply_balance_index &lt; 95 ~ \"Undersupply\",\n      supply_balance_index &lt; 105 ~ \"Balanced Growth\",\n      supply_balance_index &lt; 120 ~ \"Surplus Construction\",\n      TRUE ~ \"High Surplus\"\n    )\n  )\n\ncat(\"Supply-Growth Balance Statistics:\\n\")\n\n\nSupply-Growth Balance Statistics:\n\n\nShow Code\ncat(\"Supply/Growth Ratio (raw):\\n\")\n\n\nSupply/Growth Ratio (raw):\n\n\nShow Code\nsummary(housing_growth$supply_growth_ratio_capped)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.01713 0.40625 0.66173 1.85578 5.00000 5.00000 \n\n\nShow Code\ncat(\"\\nSupply Balance Index:\\n\")\n\n\n\nSupply Balance Index:\n\n\nShow Code\nsummary(housing_growth$supply_balance_index)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   -Inf    -Inf    -Inf             Inf     Inf \n\n\nThe Supply Balance Index measures whether housing construction is keeping up with population growth: - Index = 100: Building exactly matches population growth (1 unit per new resident) - Index &gt; 100: Building faster than population growth (reducing housing scarcity) - Index &lt; 100: Building slower than population growth (increasing housing scarcity)\n\n\nTop Performers on Each Metric (2023)\nLet’s identify metros excelling on each dimension:\n\n\nShow Code\n# Get most recent year\nmost_recent_year &lt;- max(housing_growth$year)\n\n# Top 15 by Construction Intensity\ntop_construction_intensity &lt;- housing_growth |&gt;\n  filter(year == most_recent_year) |&gt;\n  arrange(desc(construction_intensity_index)) |&gt;\n  select(NAME, population, new_housing_units_permitted, permits_per_1000, \n         construction_intensity_index) |&gt;\n  head(15)\n\nlibrary(DT)\n\ndatatable(\n  top_construction_intensity,\n  rownames = FALSE,\n  options = list(pageLength = 15, dom = 't'),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold; font-size: 14px;',\n    paste0('Top 15 Metro Areas by Construction Intensity (', most_recent_year, ')')\n  )\n) |&gt;\n  formatCurrency(\"population\", digits = 0) |&gt;\n  formatCurrency(\"new_housing_units_permitted\", digits = 0, currency = \"\", before = FALSE) |&gt;\n  formatRound(c(\"permits_per_1000\", \"construction_intensity_index\"), digits = 2) |&gt;\n  formatStyle(\n    'construction_intensity_index',\n    background = styleColorBar(range(top_construction_intensity$construction_intensity_index), 'lightblue'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\n\n\n\n\nKey Insight - Construction Intensity: These metros are building at the highest per-capita rates. Many of them are fast-growing Sunbelt cities with permissive zoning, but the list also includes some smaller metros with particularly pro-development policies.\n\n\nShow Code\n# Top 15 by Supply-Growth Balance\ntop_supply_balance &lt;- housing_growth |&gt;\n  filter(year == most_recent_year) |&gt;\n  arrange(desc(supply_balance_index)) |&gt;\n  select(NAME, pop_change_5yr, new_housing_units_permitted, \n         supply_growth_ratio_capped, supply_balance_index) |&gt;\n  head(15)\n\ndatatable(\n  top_supply_balance,\n  rownames = FALSE,\n  options = list(pageLength = 15, dom = 't'),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold; font-size: 14px;',\n    paste0('Top 15 Metro Areas by Supply-Growth Balance (', most_recent_year, ')')\n  )\n) |&gt;\n  formatCurrency(c(\"pop_change_5yr\", \"new_housing_units_permitted\"), \n                 digits = 0, currency = \"\", before = FALSE) |&gt;\n  formatRound(c(\"supply_growth_ratio_capped\", \"supply_balance_index\"), digits = 2) |&gt;\n  formatStyle(\n    'supply_balance_index',\n    background = styleColorBar(range(top_supply_balance$supply_balance_index), 'lightgreen'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\n\n\n\n\nKey Insight - Supply Balance: These metros are not just building, they’re building faster than their populations are growing. This is the hallmark of YIMBY policy: creating enough housing to improve affordability even as the metro grows.\n\n\nWorst Performers (Undersupplying Markets)\n\n\nShow Code\n# Bottom 15 by Supply-Growth Balance (most undersupplied)\nworst_supply_balance &lt;- housing_growth |&gt;\n  filter(year == most_recent_year) |&gt;\n  arrange(supply_balance_index) |&gt;\n  select(NAME, pop_change_5yr, new_housing_units_permitted, \n         supply_growth_ratio_capped, supply_balance_index) |&gt;\n  head(15)\n\ndatatable(\n  worst_supply_balance,\n  rownames = FALSE,\n  options = list(pageLength = 15, dom = 't'),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold; font-size: 14px;',\n    paste0('Most Undersupplied Metro Areas (', most_recent_year, ')')\n  )\n) |&gt;\n  formatCurrency(c(\"pop_change_5yr\", \"new_housing_units_permitted\"), \n                 digits = 0, currency = \"\", before = FALSE) |&gt;\n  formatRound(c(\"supply_growth_ratio_capped\", \"supply_balance_index\"), digits = 2) |&gt;\n  formatStyle(\n    'supply_balance_index',\n    background = styleColorBar(range(worst_supply_balance$supply_balance_index), 'coral'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\n\n\n\n\nKey Insight - Undersupply: These metros represent the housing affordability crisis. They’re experiencing population growth but failing to permit enough new housing, leading to price increases and displacement.\n\n\nComposite YIMBY Score\nFinally, we combine both metrics into a single YIMBY Score that captures both construction intensity and supply-growth balance:\n\n\nShow Code\n# Create composite score as weighted average\n# Weight construction intensity 40%, supply balance 60%\n# Reasoning: Keeping up with growth (balance) is more important than raw volume\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    yimby_score = (0.4 * construction_intensity_index) + \n                  (0.6 * supply_balance_index),\n    yimby_category = case_when(\n      yimby_score &lt; 80 ~ \"Anti-Housing (NIMBY)\",\n      yimby_score &lt; 95 ~ \"Below Average\",\n      yimby_score &lt; 105 ~ \"Average\",\n      yimby_score &lt; 120 ~ \"Pro-Housing\",\n      TRUE ~ \"Highly Pro-Housing (YIMBY)\"\n    )\n  )\n\n# Top 20 YIMBY metros\ntop_yimby_metros &lt;- housing_growth |&gt;\n  filter(year == most_recent_year) |&gt;\n  arrange(desc(yimby_score)) |&gt;\n  select(NAME, population, new_housing_units_permitted, \n         construction_intensity_index, supply_balance_index, yimby_score, yimby_category) |&gt;\n  head(20)\n\ndatatable(\n  top_yimby_metros,\n  rownames = FALSE,\n  options = list(pageLength = 20, dom = 't'),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold; font-size: 14px;',\n    paste0('Top 20 Most YIMBY-Friendly Metro Areas (', most_recent_year, ')')\n  )\n) |&gt;\n  formatCurrency(\"population\", digits = 0) |&gt;\n  formatCurrency(\"new_housing_units_permitted\", digits = 0, currency = \"\", before = FALSE) |&gt;\n  formatRound(c(\"construction_intensity_index\", \"supply_balance_index\", \"yimby_score\"), \n              digits = 1) |&gt;\n  formatStyle(\n    'yimby_score',\n    background = styleColorBar(range(top_yimby_metros$yimby_score), 'lightgreen'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  ) |&gt;\n  formatStyle(\n    'yimby_category',\n    backgroundColor = styleEqual(\n      levels = c(\"Anti-Housing (NIMBY)\", \"Below Average\", \"Average\", \n                 \"Pro-Housing\", \"Highly Pro-Housing (YIMBY)\"),\n      values = c('#f8d7da', '#fff3cd', '#d1ecf1', '#d4edda', '#28a745')\n    )\n  )\n\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\n\n\n\n\nAmerica’s Most YIMBY Cities: The top YIMBY metros in 2023 share common characteristics: - Permissive zoning: Fewer restrictions on what and where to build - Available land: Room to expand, often in Sunbelt locations - Pro-growth policies: Local governments that welcome development - Economic dynamism: Growing job markets attracting new residents - Political will: Leadership committed to housing affordability\nThe winner, Akron, OH Metro Area, achieved a YIMBY score of , demonstrating exceptional commitment to housing supply.\n\n\nVisualization: YIMBY Score Distribution\n\n\nShow Code\n# Create scatter plot showing both dimensions\nplot_data &lt;- housing_growth |&gt;\n  filter(year == most_recent_year, population &gt;= 100000)  # Focus on larger metros\n\nggplot(plot_data, aes(x = construction_intensity_index, y = supply_balance_index, \n                      size = population, color = yimby_category)) +\n  geom_point(alpha = 0.6) +\n  # Add reference lines at average (100)\n  geom_hline(yintercept = 100, linetype = \"dashed\", color = \"gray40\") +\n  geom_vline(xintercept = 100, linetype = \"dashed\", color = \"gray40\") +\n  # Add quadrant labels\n  annotate(\"text\", x = 50, y = 150, label = \"Low Volume,\\nHigh Balance\", \n           color = \"gray50\", size = 3) +\n  annotate(\"text\", x = 150, y = 150, label = \"High Volume,\\nHigh Balance\\n(YIMBY)\", \n           color = \"darkgreen\", size = 3.5, fontface = \"bold\") +\n  annotate(\"text\", x = 50, y = 50, label = \"Low Volume,\\nLow Balance\\n(NIMBY)\", \n           color = \"darkred\", size = 3.5, fontface = \"bold\") +\n  annotate(\"text\", x = 150, y = 50, label = \"High Volume,\\nLow Balance\", \n           color = \"gray50\", size = 3) +\n  # Styling\n  scale_size_continuous(range = c(2, 12), labels = scales::comma, name = \"Population\") +\n  scale_color_manual(\n    values = c(\"Anti-Housing (NIMBY)\" = \"#dc3545\", \n               \"Below Average\" = \"#ffc107\",\n               \"Average\" = \"#17a2b8\",\n               \"Pro-Housing\" = \"#28a745\",\n               \"Highly Pro-Housing (YIMBY)\" = \"#155724\"),\n    name = \"YIMBY Category\"\n  ) +\n  labs(\n    title = \"The YIMBY Landscape: Construction Intensity vs. Supply-Growth Balance\",\n    subtitle = paste0(\"Metro areas with population &gt; 100,000 (\", most_recent_year, \")\"),\n    x = \"Construction Intensity Index\\n(Housing permits per capita, 100 = median)\",\n    y = \"Supply-Growth Balance Index\\n(Construction vs. population growth, 100 = balanced)\",\n    caption = \"Data Sources: US Census Bureau Building Permits Survey & American Community Survey\\nTop-right quadrant represents ideal YIMBY policies: high construction that exceeds population growth\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"right\",\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nObservation: The ideal YIMBY metros appear in the top-right quadrant: high construction intensity and building faster than population growth. These are the cities successfully addressing housing affordability through supply-side policies. Conversely, metros in the bottom-left quadrant face the steepest challenges: low construction despite population growth, a recipe for affordability crises."
  },
  {
    "objectID": "mp02.html#task-6-identifying-yimby-success-stories",
    "href": "mp02.html#task-6-identifying-yimby-success-stories",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "7 Task 6: Identifying YIMBY Success Stories",
    "text": "7 Task 6: Identifying YIMBY Success Stories\nThe true test of YIMBY policies is not just building housing, but doing so in a way that improves affordability in growing metros. We’re now combining our Rent Burden and Housing Growth metrics to identify cities that have successfully addressed housing challenges.\n\nDefining YIMBY Success\nA successful YIMBY city must demonstrate: 1. Initial affordability challenge: High rent burden in the early study period (2009-2014) 2. Affordability improvement: Declining rent burden over time 3. Economic vitality: Population growth (not just cheap due to decline) 4. Pro-housing policies: Above-average housing construction relative to growth\n\n\nData Preparation: Combining Metrics\n\n\nShow Code\n# Combine rent burden and housing growth metrics\n# Start with rent burden for all years\nyimby_analysis &lt;- rent_burden_index |&gt;\n  select(GEOID, NAME, year, household_income, monthly_rent, \n         rent_to_income_pct, rent_burden_index, affordability_category)\n\n# Add housing growth metrics (available 2014+)\nyimby_analysis &lt;- yimby_analysis |&gt;\n  left_join(\n    housing_growth |&gt; \n      select(GEOID, year, population, new_housing_units_permitted,\n             pop_change_5yr, pop_growth_rate_5yr, permits_per_1000,\n             construction_intensity_index, supply_balance_index, yimby_score),\n    by = c(\"GEOID\", \"year\")\n  )\n\n# Calculate key metrics for identifying YIMBY success\nyimby_metrics &lt;- yimby_analysis |&gt;\n  group_by(GEOID, NAME) |&gt;\n  arrange(year) |&gt;\n  summarize(\n    # Criterion 1: Initial rent burden (average 2009-2014)\n    early_rent_burden = mean(rent_burden_index[year &gt;= 2009 & year &lt;= 2014], na.rm = TRUE),\n    \n    # Criterion 2: Change in rent burden (2014 to most recent year with data)\n    initial_rent_burden = rent_burden_index[year == 2014][1],\n    final_rent_burden = rent_burden_index[year == max(year)][1],\n    rent_burden_change = final_rent_burden - initial_rent_burden,\n    rent_burden_pct_change = (rent_burden_change / initial_rent_burden) * 100,\n    \n    # Criterion 3: Population growth (2014 to most recent)\n    initial_population = population[year == 2014][1],\n    final_population = population[year == max(year)][1],\n    total_pop_growth = final_population - initial_population,\n    pop_growth_pct = (total_pop_growth / initial_population) * 100,\n    \n    # Criterion 4: Average housing growth metrics (2014+)\n    avg_yimby_score = mean(yimby_score[year &gt;= 2014], na.rm = TRUE),\n    avg_construction_intensity = mean(construction_intensity_index[year &gt;= 2014], na.rm = TRUE),\n    avg_supply_balance = mean(supply_balance_index[year &gt;= 2014], na.rm = TRUE),\n    \n    # Additional context\n    final_year = max(year),\n    \n    .groups = \"drop\"\n  ) |&gt;\n  # Remove metros with insufficient data\n  filter(!is.na(early_rent_burden), \n         !is.na(rent_burden_change),\n         !is.na(pop_growth_pct),\n         !is.na(avg_yimby_score))\n\n# Identify YIMBY success stories based on all 4 criteria\nyimby_success &lt;- yimby_metrics |&gt;\n  mutate(\n    # Criterion 1: High initial burden (above 105)\n    had_high_burden = early_rent_burden &gt; 105,\n    \n    # Criterion 2: Rent burden decreased (negative change)\n    burden_decreased = rent_burden_change &lt; 0,\n    \n    # Criterion 3: Population grew (positive growth)\n    population_grew = pop_growth_pct &gt; 0,\n    \n    # Criterion 4: Above-average housing growth (YIMBY score &gt; 100)\n    strong_housing_growth = avg_yimby_score &gt; 100,\n    \n    # Count criteria met\n    criteria_met = had_high_burden + burden_decreased + population_grew + strong_housing_growth,\n    \n    # Classify metros\n    yimby_classification = case_when(\n      criteria_met == 4 ~ \"YIMBY Success Story\",\n      criteria_met == 3 & strong_housing_growth ~ \"Emerging YIMBY\",\n      criteria_met &gt;= 2 & population_grew ~ \"Mixed Results\",\n      population_grew == FALSE ~ \"Declining/Stagnant\",\n      TRUE ~ \"NIMBY Challenge\"\n    )\n  )\n\n# Summary of classifications\ncat(\"YIMBY Classification Summary:\\n\")\n\n\nYIMBY Classification Summary:\n\n\nShow Code\ntable(yimby_success$yimby_classification)\n\n\n\n Declining/Stagnant      Emerging YIMBY       Mixed Results     NIMBY Challenge \n                 24                   3                  43                  36 \nYIMBY Success Story \n                  1 \n\n\nWe identified 1 metros that meet all four YIMBY success criteria, demonstrating that pro-housing policies can improve affordability even in growing dynamic cities economically.\n\n\nVisualization 1: The YIMBY Success Quadrant\nThis visualization plots rent burden change against housing growth, with bubble size representing population growth:\n\n\nShow Code\n# Create scatter plot showing relationship between rent burden change and housing growth\n# Filter to metros with population &gt; 100,000 for clarity\nplot_data &lt;- yimby_success |&gt;\n  filter(final_population &gt;= 100000) |&gt;\n  mutate(\n    pop_size_category = cut(final_population, \n                            breaks = c(0, 250000, 500000, 1000000, Inf),\n                            labels = c(\"&lt;250K\", \"250K-500K\", \"500K-1M\", \"&gt;1M\"))\n  )\n\nggplot(plot_data, aes(x = avg_yimby_score, y = rent_burden_change, \n                      size = pop_growth_pct, color = yimby_classification)) +\n  # Add reference lines\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray40\", linewidth = 0.8) +\n  geom_vline(xintercept = 100, linetype = \"dashed\", color = \"gray40\", linewidth = 0.8) +\n  \n  # Add quadrant labels\n  annotate(\"rect\", xmin = 100, xmax = Inf, ymin = -Inf, ymax = 0,\n           alpha = 0.1, fill = \"green\") +\n  annotate(\"text\", x = 130, y = -15, \n           label = \"YIMBY SUCCESS\\n(High construction,\\nimproving affordability)\", \n           color = \"darkgreen\", size = 4, fontface = \"bold\") +\n  \n  annotate(\"text\", x = 70, y = -15, \n           label = \"Declining Markets\\n(Improving affordability\\ndue to population loss)\", \n           color = \"gray50\", size = 3) +\n  \n  annotate(\"text\", x = 130, y = 15, \n           label = \"Building but\\nNot Enough\\n(Construction failing\\nto improve affordability)\", \n           color = \"orange\", size = 3) +\n  \n  annotate(\"text\", x = 70, y = 15, \n           label = \"NIMBY Crisis\\n(Low construction,\\nworsening affordability)\", \n           color = \"darkred\", size = 3.5, fontface = \"bold\") +\n  \n  # Points\n  geom_point(alpha = 0.7) +\n  \n  # Styling\n  scale_size_continuous(\n    range = c(3, 15), \n    labels = scales::percent_format(scale = 1),\n    name = \"Population Growth %\"\n  ) +\n  scale_color_manual(\n    values = c(\n      \"YIMBY Success Story\" = \"#28a745\",\n      \"Emerging YIMBY\" = \"#5cb85c\",\n      \"Mixed Results\" = \"#ffc107\",\n      \"Declining/Stagnant\" = \"#6c757d\",\n      \"NIMBY Challenge\" = \"#dc3545\"\n    ),\n    name = \"Classification\"\n  ) +\n  scale_x_continuous(breaks = seq(60, 160, 20)) +\n  scale_y_continuous(breaks = seq(-30, 30, 10)) +\n  \n  labs(\n    title = \"The YIMBY Success Landscape: Housing Growth vs. Affordability Improvement\",\n    subtitle = \"Metro areas with population &gt; 100,000 (2014-2023)\",\n    x = \"Average YIMBY Score (2014-2023)\\n← Less Construction | More Construction →\",\n    y = \"Change in Rent Burden Index (2014-2023)\\n← More Affordable | Less Affordable →\",\n    caption = \"Data Sources: US Census Bureau & American Community Survey\\nGreen quadrant (top-right) shows ideal outcome: high housing construction improving affordability in growing metros\"\n  ) +\n  \n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 15),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\", size = 11),\n    legend.position = \"right\",\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nObservation: The green-shaded quadrant (upper-right) represents the YIMBY success zone: metros with above-average housing construction that have seen rent burden decrease over time. The size of bubbles shows population growth, confirming these are economically vibrant areas, not cities declining into affordability. The lower-left quadrant shows the opposite: NIMBY metros with low construction and worsening affordability crises.\n\n\nVisualization 2: YIMBY Success Stories Over Time\nLet’s examine how rent burden has evolved in the most successful YIMBY metros:\n\n\nShow Code\n# Identify top 10 YIMBY success stories\ntop_yimby_success &lt;- yimby_success |&gt;\n  filter(yimby_classification == \"YIMBY Success Story\") |&gt;\n  arrange(desc(avg_yimby_score), rent_burden_change) |&gt;\n  head(10)\n\n# Get time series for these metros\nyimby_success_timeseries &lt;- yimby_analysis |&gt;\n  filter(GEOID %in% top_yimby_success$GEOID) |&gt;\n  mutate(\n    short_name = str_extract(NAME, \"^[^,]+\")  # Extract city name\n  )\n\n# Also get some NIMBY comparison metros (high burden, got worse, growing population)\nnimby_comparison &lt;- yimby_success |&gt;\n  filter(early_rent_burden &gt; 105,  # Started with high burden\n         rent_burden_change &gt; 5,    # Got worse\n         pop_growth_pct &gt; 5,        # Still growing\n         final_population &gt;= 500000) |&gt;  # Large metros\n  arrange(desc(rent_burden_change)) |&gt;\n  head(5)\n\nnimby_timeseries &lt;- yimby_analysis |&gt;\n  filter(GEOID %in% nimby_comparison$GEOID) |&gt;\n  mutate(short_name = str_extract(NAME, \"^[^,]+\"))\n\n# Combine for visualization\ncomparison_data &lt;- bind_rows(\n  yimby_success_timeseries |&gt; mutate(category = \"YIMBY Success\"),\n  nimby_timeseries |&gt; mutate(category = \"NIMBY Comparison\")\n)\n\nlibrary(gghighlight)\n\nggplot(comparison_data, aes(x = year, y = rent_burden_index, \n                            group = GEOID, color = category)) +\n  geom_line(linewidth = 1.2, alpha = 0.8) +\n  geom_hline(yintercept = 100, linetype = \"dashed\", color = \"gray40\", linewidth = 0.8) +\n  annotate(\"text\", x = 2010, y = 102, label = \"2009 National Average\", \n           color = \"gray40\", size = 3) +\n  \n  # Use faceting to separate YIMBY vs NIMBY\n  facet_wrap(~category, ncol = 1) +\n  \n  # Styling\n  scale_color_manual(\n    values = c(\"YIMBY Success\" = \"#28a745\", \"NIMBY Comparison\" = \"#dc3545\"),\n    name = \"Policy Approach\"\n  ) +\n  scale_y_continuous(breaks = seq(80, 160, 20)) +\n  scale_x_continuous(breaks = seq(2009, 2023, 2)) +\n  \n  labs(\n    title = \"YIMBY Success vs. NIMBY Failure: Rent Burden Trajectories (2009-2023)\",\n    subtitle = \"Top 10 YIMBY metros show improving affordability despite growth; NIMBY metros show worsening crisis\",\n    x = \"Year\",\n    y = \"Rent Burden Index\\n(100 = 2009 National Average)\",\n    caption = \"Data Source: American Community Survey\\nYIMBY metros combined high housing construction with population growth and declining rent burden\"\n  ) +\n  \n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"top\",\n    strip.text = element_text(face = \"bold\", size = 12),\n    strip.background = element_rect(fill = \"gray90\", color = NA),\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nObservation: This comparison frankly illustrates the impact of housing policy. The top panel shows YIMBY success stories: metros that started with high rent burdens but have seen steady improvement through aggressive housing construction. The bottom panel shows NIMBY metros: despite (or because of) similar growth pressures, these cities have failed to build adequate housing, resulting in worsening affordability crises.\n\n\nThe YIMBY Success Honor Roll\nHere are the metros that successfully met all four YIMBY criteria:\n\n\nShow Code\n# Create detailed table of YIMBY success stories\nyimby_success_table &lt;- yimby_success |&gt;\n  filter(yimby_classification == \"YIMBY Success Story\") |&gt;\n  arrange(desc(avg_yimby_score)) |&gt;\n  select(NAME, early_rent_burden, final_rent_burden, rent_burden_change,\n         pop_growth_pct, avg_yimby_score) |&gt;\n  mutate(\n    early_rent_burden = round(early_rent_burden, 1),\n    final_rent_burden = round(final_rent_burden, 1),\n    rent_burden_change = round(rent_burden_change, 1),\n    pop_growth_pct = round(pop_growth_pct, 1),\n    avg_yimby_score = round(avg_yimby_score, 1)\n  )\n\nlibrary(DT)\n\ndatatable(\n  yimby_success_table,\n  rownames = FALSE,\n  colnames = c(\"Metro Area\", \"Initial Rent Burden\\n(2009-2014 Avg)\", \n               \"Current Rent Burden\", \"Burden Change\", \n               \"Population Growth %\", \"Avg YIMBY Score\"),\n  options = list(\n    pageLength = 20,\n    order = list(list(5, 'desc')),  # Sort by YIMBY score\n    dom = 'tp'\n  ),\n  caption = htmltools::tags$caption(\n    style = 'caption-side: top; text-align: left; font-weight: bold; font-size: 14px;',\n    'YIMBY Success Stories: Metros Meeting All Four Criteria'\n  )\n) |&gt;\n  formatStyle(\n    'rent_burden_change',\n    background = styleInterval(c(-10, -5, 0), c('#155724', '#28a745', '#d4edda', '#fff3cd')),\n    color = styleInterval(-10, c('white', 'black'))\n  ) |&gt;\n  formatStyle(\n    'avg_yimby_score',\n    background = styleColorBar(range(yimby_success_table$avg_yimby_score), '#28a745'),\n    backgroundSize = '100% 90%',\n    backgroundRepeat = 'no-repeat',\n    backgroundPosition = 'center'\n  )\n\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\n\n\n\n\nKey Findings: These 1 metro areas demonstrate that pro-housing policies work. Common characteristics include:\n\nPermissive zoning: Allowing multi-family housing and reducing parking requirements\nStreamlined permitting: Faster approval processes reduce construction costs\nPolitical will: Local leadership prioritizing affordability over NIMBYism\nAvailable land: Room to expand, particularly in Sunbelt locations\nEconomic growth: Strong job markets creating demand for housing\n\nThe leader, Ocean City, NJ Metro Area, achieved an average YIMBY score of  while reducing rent burden by 21.4 points and growing its population by 0.3%. This is the model other metros should follow.\n\n\nPolicy Implications\nThe existence of YIMBY success stories proves that housing affordability crises are not inevitable. Cities can grow, remain economically vibrant, and improve affordability simultaneously, but only if they embrace supply-side housing policies. The alternative, visible in NIMBY metros, is a vicious cycle: restricted supply drives up prices, which increases political pressure for rent control and other demand-side interventions that further discourage construction.\nFor federal policymakers considering YIMBY incentive programs, these success stories provide compelling evidence: housing supply matters, and cities that build housing see real affordability improvements."
  },
  {
    "objectID": "mp02.html#task-7-policy-brief-development",
    "href": "mp02.html#task-7-policy-brief-development",
    "title": "Mini-Project 02 - Making Backyards Affordable for All",
    "section": "8 Task 7: Policy Brief Development",
    "text": "8 Task 7: Policy Brief Development\n\nIdentifying Congressional Sponsors\n\n\nShow Code\n# Find the best YIMBY success story (primary sponsor)\nprimary_sponsor_city &lt;- yimby_success |&gt;\n  filter(yimby_classification == \"YIMBY Success Story\",\n         final_population &gt;= 500000) |&gt;  # Large enough to have congressional representation\n  arrange(desc(avg_yimby_score)) |&gt;\n  slice(1)\n\ncat(\"PRIMARY SPONSOR CITY:\\n\")\n\n\nPRIMARY SPONSOR CITY:\n\n\nShow Code\ncat(\"Metro:\", primary_sponsor_city$NAME, \"\\n\")\n\n\nMetro:  \n\n\nShow Code\ncat(\"YIMBY Score:\", round(primary_sponsor_city$avg_yimby_score, 1), \"\\n\")\n\n\nYIMBY Score:  \n\n\nShow Code\ncat(\"Rent Burden Improvement:\", round(primary_sponsor_city$rent_burden_change, 1), \"points\\n\")\n\n\nRent Burden Improvement:  points\n\n\nShow Code\ncat(\"Population Growth:\", round(primary_sponsor_city$pop_growth_pct, 1), \"%\\n\\n\")\n\n\nPopulation Growth:  %\n\n\nShow Code\n# Find a high-rent, low-construction NIMBY city (co-sponsor)\ncosponsor_city &lt;- yimby_success |&gt;\n  filter(early_rent_burden &gt; 110,  # High rent burden\n         rent_burden_change &gt; 0,    # Getting worse\n         avg_yimby_score &lt; 95,      # Low housing growth\n         pop_growth_pct &gt; 5,        # Still growing (shows need)\n         final_population &gt;= 1000000) |&gt;  # Major metro\n  arrange(desc(early_rent_burden)) |&gt;\n  slice(1)\n\ncat(\"CO-SPONSOR CITY:\\n\")\n\n\nCO-SPONSOR CITY:\n\n\nShow Code\ncat(\"Metro:\", cosponsor_city$NAME, \"\\n\")\n\n\nMetro: Tampa-St. Petersburg-Clearwater, FL Metro Area \n\n\nShow Code\ncat(\"Current Rent Burden:\", round(cosponsor_city$final_rent_burden, 1), \"\\n\")\n\n\nCurrent Rent Burden: 142.3 \n\n\nShow Code\ncat(\"YIMBY Score:\", round(cosponsor_city$avg_yimby_score, 1), \"(below average)\\n\")\n\n\nYIMBY Score: -Inf (below average)\n\n\nShow Code\ncat(\"Population Growth:\", round(cosponsor_city$pop_growth_pct, 1), \"%\\n\")\n\n\nPopulation Growth: 14.7 %\n\n\n\n\nIdentifying Congressional Sponsors\n\n\nShow Code\n# Find the best YIMBY success story (primary sponsor)\n# First try strict criteria\nprimary_sponsor_city &lt;- yimby_success |&gt;\n  filter(yimby_classification == \"YIMBY Success Story\",\n         final_population &gt;= 500000) |&gt;\n  arrange(desc(avg_yimby_score)) |&gt;\n  slice(1)\n\n# If no cities meet strict criteria, relax it\nif(nrow(primary_sponsor_city) == 0) {\n  cat(\"No YIMBY Success Stories with 500K+ population. Relaxing criteria...\\n\")\n  \n  primary_sponsor_city &lt;- yimby_success |&gt;\n    filter(yimby_classification %in% c(\"YIMBY Success Story\", \"Emerging YIMBY\"),\n           final_population &gt;= 250000) |&gt;\n    arrange(desc(avg_yimby_score)) |&gt;\n    slice(1)\n}\n\n\nNo YIMBY Success Stories with 500K+ population. Relaxing criteria...\n\n\nShow Code\n# If still nothing, just take the highest YIMBY score with growth\nif(nrow(primary_sponsor_city) == 0) {\n  cat(\"Still no matches. Taking highest YIMBY score among growing metros...\\n\")\n  \n  primary_sponsor_city &lt;- yimby_success |&gt;\n    filter(population_grew == TRUE,\n           strong_housing_growth == TRUE,\n           final_population &gt;= 250000) |&gt;\n    arrange(desc(avg_yimby_score)) |&gt;\n    slice(1)\n}\n\ncat(\"PRIMARY SPONSOR CITY:\\n\")\n\n\nPRIMARY SPONSOR CITY:\n\n\nShow Code\ncat(\"Metro:\", primary_sponsor_city$NAME, \"\\n\")\n\n\nMetro: Dayton, OH Metro Area \n\n\nShow Code\ncat(\"YIMBY Score:\", round(primary_sponsor_city$avg_yimby_score, 1), \"\\n\")\n\n\nYIMBY Score: Inf \n\n\nShow Code\ncat(\"Rent Burden Change:\", round(primary_sponsor_city$rent_burden_change, 1), \"points\\n\")\n\n\nRent Burden Change: -11 points\n\n\nShow Code\ncat(\"Population Growth:\", round(primary_sponsor_city$pop_growth_pct, 1), \"%\\n\")\n\n\nPopulation Growth: 0.7 %\n\n\nShow Code\ncat(\"Classification:\", primary_sponsor_city$yimby_classification, \"\\n\\n\")\n\n\nClassification: Emerging YIMBY \n\n\nShow Code\n# Find a high-rent, low-construction NIMBY city (co-sponsor)\ncosponsor_city &lt;- yimby_success |&gt;\n  filter(early_rent_burden &gt; 110,\n         rent_burden_change &gt; 0,\n         avg_yimby_score &lt; 95,\n         pop_growth_pct &gt; 5,\n         final_population &gt;= 500000) |&gt;\n  arrange(desc(early_rent_burden)) |&gt;\n  slice(1)\n\n# If no city meets all criteria, relax them\nif(nrow(cosponsor_city) == 0) {\n  cat(\"No cities meet strict NIMBY criteria. Relaxing...\\n\")\n  \n  cosponsor_city &lt;- yimby_success |&gt;\n    filter(early_rent_burden &gt; 105,  # Lower threshold\n           avg_yimby_score &lt; 100,     # Just below average\n           final_population &gt;= 500000) |&gt;\n    arrange(desc(final_rent_burden)) |&gt;\n    slice(1)\n}\n\ncat(\"CO-SPONSOR CITY:\\n\")\n\n\nCO-SPONSOR CITY:\n\n\nShow Code\ncat(\"Metro:\", cosponsor_city$NAME, \"\\n\")\n\n\nMetro: Deltona-Daytona Beach-Ormond Beach, FL Metro Area \n\n\nShow Code\ncat(\"Current Rent Burden:\", round(cosponsor_city$final_rent_burden, 1), \"\\n\")\n\n\nCurrent Rent Burden: 135.1 \n\n\nShow Code\ncat(\"YIMBY Score:\", round(cosponsor_city$avg_yimby_score, 1), \"\\n\")\n\n\nYIMBY Score: -Inf \n\n\nShow Code\ncat(\"Population Growth:\", round(cosponsor_city$pop_growth_pct, 1), \"%\\n\")\n\n\nPopulation Growth: 18.3 %\n\n\nShow Code\ncat(\"Classification:\", cosponsor_city$yimby_classification, \"\\n\")\n\n\nClassification: Mixed Results \n\n\n\n\nIdentifying Key Occupational Groups\nNow we’ll analyze employment data to find occupations important in both cities.\n\n\nShow Code\n# Get CBSA codes for our sponsor cities\nprimary_cbsa &lt;- primary_sponsor_city$GEOID\ncosponsor_cbsa &lt;- cosponsor_city$GEOID\n\ncat(\"Analyzing occupations for:\\n\")\n\n\nAnalyzing occupations for:\n\n\nShow Code\ncat(\"Primary city:\", primary_sponsor_city$NAME, \"(CBSA:\", primary_cbsa, \")\\n\")\n\n\nPrimary city: Dayton, OH Metro Area (CBSA: 19380 )\n\n\nShow Code\ncat(\"Co-sponsor city:\", cosponsor_city$NAME, \"(CBSA:\", cosponsor_cbsa, \")\\n\\n\")\n\n\nCo-sponsor city: Deltona-Daytona Beach-Ormond Beach, FL Metro Area (CBSA: 19660 )\n\n\nShow Code\n# Prepare WAGES data\nWAGES_for_analysis &lt;- WAGES |&gt;\n  mutate(CBSA_code = as.numeric(str_remove(FIPS, \"C\")) * 10,\n         level2_code = floor(INDUSTRY / 100))\n\n# Get most recent year\nrecent_year &lt;- max(WAGES_for_analysis$YEAR)\n\n# Get employment data for both cities\nboth_cities_employment &lt;- WAGES_for_analysis |&gt;\n  filter(CBSA_code %in% c(primary_cbsa, cosponsor_cbsa),\n         YEAR == recent_year) |&gt;\n  left_join(\n    INDUSTRY_CODES |&gt; \n      select(level2_code, level2_title) |&gt; \n      distinct(),\n    by = \"level2_code\"\n  ) |&gt;\n  filter(!is.na(level2_title)) |&gt;\n  group_by(CBSA_code, level2_title) |&gt;\n  summarize(\n    total_employment = sum(EMPLOYMENT, na.rm = TRUE),\n    avg_wage = weighted.mean(AVG_WAGE, w = EMPLOYMENT, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Separate data for each city\nprimary_city_data &lt;- both_cities_employment |&gt;\n  filter(CBSA_code == primary_cbsa) |&gt;\n  mutate(employment_share = total_employment / sum(total_employment) * 100) |&gt;\n  rename(primary_employment = total_employment,\n         primary_wage = avg_wage,\n         primary_share = employment_share)\n\ncosponsor_city_data &lt;- both_cities_employment |&gt;\n  filter(CBSA_code == cosponsor_cbsa) |&gt;\n  mutate(employment_share = total_employment / sum(total_employment) * 100) |&gt;\n  rename(cosponsor_employment = total_employment,\n         cosponsor_wage = avg_wage,\n         cosponsor_share = employment_share)\n\n# Join to find common occupations\ncommon_sectors &lt;- primary_city_data |&gt;\n  inner_join(cosponsor_city_data, by = \"level2_title\") |&gt;\n  filter(primary_share &gt;= 2, cosponsor_share &gt;= 2) |&gt;  # At least 2% in both cities\n  mutate(\n    wage_diff = cosponsor_wage - primary_wage,\n    wage_diff_pct = (wage_diff / primary_wage) * 100,\n    total_employment = primary_employment + cosponsor_employment\n  ) |&gt;\n  arrange(desc(wage_diff_pct))\n\n# If no sectors meet 2% threshold, lower it\nif(nrow(common_sectors) == 0) {\n  cat(\"No sectors with 2%+ share in both cities. Lowering threshold to 1%...\\n\\n\")\n  \n  common_sectors &lt;- primary_city_data |&gt;\n    inner_join(cosponsor_city_data, by = \"level2_title\") |&gt;\n    filter(primary_share &gt;= 1, cosponsor_share &gt;= 1) |&gt;\n    mutate(\n      wage_diff = cosponsor_wage - primary_wage,\n      wage_diff_pct = (wage_diff / primary_wage) * 100,\n      total_employment = primary_employment + cosponsor_employment\n    ) |&gt;\n    arrange(desc(wage_diff_pct))\n}\n\n# Select occupation groups for lobbying\n# Group 1: Highest wage premium in expensive city (shows cost burden)\noccupation_1 &lt;- common_sectors |&gt; slice(1)\n\n# Group 2: Largest combined employment base\noccupation_2 &lt;- common_sectors |&gt; \n  arrange(desc(total_employment)) |&gt;\n  slice(1)\n\n# If they're the same, pick second largest for variety\nif(occupation_1$level2_title == occupation_2$level2_title) {\n  occupation_2 &lt;- common_sectors |&gt; \n    arrange(desc(total_employment)) |&gt;\n    slice(2)\n}\n\ncat(\"==============================================\\n\")\n\n\n==============================================\n\n\nShow Code\ncat(\"KEY OCCUPATIONAL GROUPS FOR LOBBYING:\\n\")\n\n\nKEY OCCUPATIONAL GROUPS FOR LOBBYING:\n\n\nShow Code\ncat(\"==============================================\\n\\n\")\n\n\n==============================================\n\n\nShow Code\ncat(\"Group 1:\", occupation_1$level2_title, \"\\n\")\n\n\nGroup 1: NAICS 722 Food services and drinking places \n\n\nShow Code\ncat(\"  - Employment in\", str_extract(primary_sponsor_city$NAME, \"^[^,]+\"), \":\", \n    format(occupation_1$primary_employment, big.mark=\",\"), \n    \"(\", round(occupation_1$primary_share, 1), \"% of workforce)\\n\")\n\n\n  - Employment in Dayton : 31,639 ( 14.8 % of workforce)\n\n\nShow Code\ncat(\"  - Employment in\", str_extract(cosponsor_city$NAME, \"^[^,]+\"), \":\", \n    format(occupation_1$cosponsor_employment, big.mark=\",\"),\n    \"(\", round(occupation_1$cosponsor_share, 1), \"% of workforce)\\n\")\n\n\n  - Employment in Deltona-Daytona Beach-Ormond Beach : 26,240 ( 19.8 % of workforce)\n\n\nShow Code\ncat(\"  - Average wage in\", str_extract(primary_sponsor_city$NAME, \"^[^,]+\"), \":\", \n    scales::dollar(occupation_1$primary_wage), \"\\n\")\n\n\n  - Average wage in Dayton : $21,449.54 \n\n\nShow Code\ncat(\"  - Average wage in\", str_extract(cosponsor_city$NAME, \"^[^,]+\"), \":\", \n    scales::dollar(occupation_1$cosponsor_wage), \"\\n\")\n\n\n  - Average wage in Deltona-Daytona Beach-Ormond Beach : $25,457.32 \n\n\nShow Code\ncat(\"  - Wage premium in expensive city:\", round(occupation_1$wage_diff_pct, 1), \"%\\n\")\n\n\n  - Wage premium in expensive city: 18.7 %\n\n\nShow Code\ncat(\"  - LOBBYING ANGLE: Workers in this sector pay\", \n    scales::dollar(abs(occupation_1$wage_diff)), \n    \"more in the expensive city, despite similar work\\n\\n\")\n\n\n  - LOBBYING ANGLE: Workers in this sector pay $4,007.78 more in the expensive city, despite similar work\n\n\nShow Code\ncat(\"Group 2:\", occupation_2$level2_title, \"\\n\")\n\n\nGroup 2: NAICS 541 Professional, scientific, and technical services \n\n\nShow Code\ncat(\"  - Combined employment:\", format(occupation_2$total_employment, big.mark=\",\"), \"\\n\")\n\n\n  - Combined employment: 34,241 \n\n\nShow Code\ncat(\"  - Employment in\", str_extract(primary_sponsor_city$NAME, \"^[^,]+\"), \":\", \n    format(occupation_2$primary_employment, big.mark=\",\"),\n    \"(\", round(occupation_2$primary_share, 1), \"% of workforce)\\n\")\n\n\n  - Employment in Dayton : 25,518 ( 12 % of workforce)\n\n\nShow Code\ncat(\"  - Employment in\", str_extract(cosponsor_city$NAME, \"^[^,]+\"), \":\", \n    format(occupation_2$cosponsor_employment, big.mark=\",\"),\n    \"(\", round(occupation_2$cosponsor_share, 1), \"% of workforce)\\n\")\n\n\n  - Employment in Deltona-Daytona Beach-Ormond Beach : 8,723 ( 6.6 % of workforce)\n\n\nShow Code\ncat(\"  - LOBBYING ANGLE: This sector represents\", \n    round(mean(c(occupation_2$primary_share, occupation_2$cosponsor_share)), 1),\n    \"% of the workforce - a politically crucial constituency\\n\")\n\n\n  - LOBBYING ANGLE: This sector represents 9.3 % of the workforce - a politically crucial constituency\n\n\nThese occupational groups represent hundreds of thousands of voters in our sponsor cities, making them politically crucial constituencies.\n#Policy Brief: Federal YIMBY Incentive Program: Building Affordable Communities Through Housing Supply\nBased on our analysis, here is the policy brief for congressional sponsors:\n\n\n\nFEDERAL YIMBY INCENTIVE PROGRAM:Building Affordable Communities Through Housing Supply\n\n\nPolicy Brief for Congressional Sponsors\n\n\n\n\n8.1 THE OPPORTUNITY\nHousing costs are crushing American families, but we have proof that pro-housing policies work. Cities that embrace YIMBY (Yes In My Back Yard) principles—permissive zoning, streamlined permitting, and abundant construction—have improved affordability while growing their economies. Federal incentives can replicate this success nationwide.\n\n\n8.2 RECOMMENDED SPONSORS\nPrimary Sponsor: Representative from Dayton\nThis metro exemplifies YIMBY success: despite strong population growth (+1%), aggressive housing construction has reduced rent burden by 11 points since 2014. Your constituents benefit from both economic growth and improving affordability—a powerful validation of pro-housing policies.\nCo-Sponsor: Representative from Deltona-Daytona Beach-Ormond Beach\nThis metro faces a severe affordability crisis: rent burden stands at 135 (vs. 100 national baseline), among the highest in the nation. Despite population growth, restrictive housing policies have worsened affordability. Your constituents desperately need federal incentives to overcome local NIMBY opposition.\n\n\n8.3 KEY CONSTITUENCIES\nNAICS 722 Food services and drinking places Workers (`(57,879 employees combined) In high-rent cities, these workers face housing costs 19% higher than in pro-housing metros. Reducing rent burden puts money directly in their pockets—equivalent to a substantial raise without any employer cost.\nNAICS 541 Professional, scientific, and technical services Sector (34,241 employees, 9.3% of workforce)\n\n\n8.4 PROGRAM METRICS\nRent Burden Index\nMeasures housing affordability by comparing median rent to median household income, normalized to national baseline (100 = 2009 average). Higher scores indicate greater rent burden. Cities showing declining burden despite growth demonstrate YIMBY success.\nHousing Growth Index\nComposite score evaluating: (1) per-capita construction rates, and (2) whether housing supply growth exceeds population growth. Scores above 100 indicate pro-housing policies; top performers build enough to reduce housing scarcity even as populations expand.\nFunding Formula: Prioritize metros that demonstrate both (a) current affordability challenges (rent burden &gt; 105) and (b) commitment to supply-side solutions (housing growth &gt; 100), or reward metros maintaining affordability through sustained construction.\n\n\n8.5 THE ASK\nEstablish competitive federal grants ($500M annually) for municipalities that: - Streamline permitting (&lt; 6 months for multifamily projects) - Eliminate single-family-only zoning - Reduce parking requirements - Increase allowed density near transit\nCities meeting benchmarks receive infrastructure funding, housing vouchers, and planning grants. Metros achieving affordability improvements qualify for bonus funding.\n\n\n8.6 WHY THIS WORKS\nWe have proof: Dayton and similar metros show that YIMBY policies improve affordability without sacrificing growth. The alternative—restrictive policies in Deltona-Daytona Beach-Ormond Beach and elsewhere—creates housing crises that hurt working families and constrain economic opportunity.\nYour constituents win either way: Pro-housing metros receive recognition and continued support; struggling metros gain tools and incentives to overcome NIMBY opposition.\n\n\nFor detailed analysis and supporting data, contact [Your Organization] Based on US Census Bureau data (2009-2023) covering 593 metropolitan areas\n\n\n\n\nThis work ©2025 by cguirand27 was initially prepared as a Mini-Project for STA 9750 at Baruch College. More details about this course can be found at the course site and instructions for this assignment can be found at MP #02"
  }
]